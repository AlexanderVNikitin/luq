{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fec8f4",
   "metadata": {},
   "source": [
    "# Working with LUQ Datasets\n",
    "\n",
    "Welcome to the LUQ Datasets tutorial!\n",
    "In this guide, you'll learn how to:\n",
    "- Create your own LUQ datasets\n",
    "- Access and use existing LUQ datasets\n",
    "\n",
    "This tutorial is designed to help you get started quickly and effectively with LUQ's dataset tools and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29fca7",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the Dataset\n",
    "\n",
    "To ensure consistency across various QA datasets, **LUQ** requires all datasets to be converted into a unified format. The required format is simple:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"question\": \"Is this an example of a question?\",\n",
    "      \"answer\": \"Yes, it is\"\n",
    "    }\n",
    "    // ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "LUQ provides a helper script to preprocess several commonly used QA datasets such as CoQA, Natural Questions (NQ), and more.\n",
    "\n",
    "ðŸ“„ You can find the script here: `scripts/process_dataset.py`.\n",
    "\n",
    "Letâ€™s walk through an example using the CoQA dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555d1f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python scripts/process_dataset.py \\\n",
    "    --dataset=coqa \\\n",
    "    --output=data/coqa/processed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f4987",
   "metadata": {},
   "source": [
    "## Step 2: Add LLM Responses to the Dataset\n",
    "\n",
    "In this step, we enhance the dataset by generating multiple responses using a Large Language Model (LLM). This is done with the script: `scripts/add_generations_to_dataset.py`.\n",
    "\n",
    "\n",
    "This script:\n",
    "- Generates multiple responses (**samples**) per question.\n",
    "- Produces a final answer (typically sampled with a low temperature).\n",
    "- Adds token-level log probabilities for downstream analysis or training tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¾ Output Format\n",
    "\n",
    "The output is a `.json` file structured as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"llm\": \"llm_name\",\n",
    "  \"raw_dataset\": \"name_or_url_of_raw_dataset\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"question\": \"Is this a question?\",\n",
    "      \"samples\": [\"Yes\", \"No\", \"Maybe so\"],\n",
    "      \"answer\": \"Yes\",\n",
    "      \"gt_answer\": \"Yes\",\n",
    "      \"samples_temp\": 1,\n",
    "      \"answer_temp\": 0.1,\n",
    "      \"top-p\": 0.95\n",
    "    }\n",
    "    // ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "ðŸ“˜ Field Descriptions\n",
    "- llm â€“ Name of the LLM used (e.g., gpt-4, llama-2)\n",
    "- raw_dataset â€“ Name or URL of the original dataset\n",
    "- data â€“ List of entries, each containing:\n",
    "    - question: The input question\n",
    "    - samples: Multiple responses generated by the LLM\n",
    "    - answer: Final selected answer (usually low-temperature)\n",
    "    - gt_answer: Ground truth answer (if available)\n",
    "    - samples_temp: Temperature used for generating samples\n",
    "    - answer_temp: Temperature used for generating the answer\n",
    "    - top-p: Top-p value (nucleus sampling)\n",
    "    - (Optional) Other generation parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311004e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python scripts/add_generations_to_dataset.py \\\n",
    "    --input-file=./data/coqa/processed_short.json\\\n",
    "    --output-file=./data/coqa/processed_gen_short.json\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0709ba",
   "metadata": {},
   "source": [
    "## Step 3: Assess the Accuracy of the Predictions\n",
    "\n",
    "In this step, we evaluate the quality of the generated answers by using an **LLM-as-a-judge** approach. This means using a language model to assess whether each prediction is correct.\n",
    "\n",
    "To do this, use the following script: `scripts/add_accuracy_to_dataset.py`.\n",
    "\n",
    "\n",
    "This script adds an **accuracy score** to each response by comparing the LLM-generated answers to the ground truth (`gt_answer`). The evaluation is typically done by prompting an LLM to judge whether each response is correct, based on the context and expected answer.\n",
    "\n",
    "---\n",
    "\n",
    "After running this step, the dataset will include additional fields such as:\n",
    "\n",
    "- `accuracy`: A score or flag indicating whether the answer is correct\n",
    "- `judgment_explanation` *(optional)*: The LLM's reasoning or justification\n",
    "\n",
    "This automatic evaluation enables large-scale analysis of model performance without requiring human annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eff05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python scripts/eval_accuracy.py \\\n",
    "    --input-file=data/coqa/processed_gen_short.json \\\n",
    "    --output-file=data/coqa/processed_gen_acc_short.json \\\n",
    "    --model-name=gpt2 \\\n",
    "    --model-type=huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05e3cb",
   "metadata": {},
   "source": [
    "## Step 4 (Optional): Upload the Dataset to Hugging Face\n",
    "\n",
    "As an optional final step, you can upload your generated dataset to **Hugging Face** to make it publicly available and easily shareable with others.\n",
    "\n",
    "To do this, use the script: `scripts/upload_dataset.py`.\n",
    "\n",
    "\n",
    "Uploading your dataset to Hugging Face allows others to explore, download, and use your data through the Hugging Face Hub, making collaboration and reproducibility easier.\n",
    "\n",
    "> ðŸ”’ Make sure you have a Hugging Face account and the appropriate API token configured before uploading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa07170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python scripts/upload_dataset.py \\\n",
    "    --path=data/coqa/processed_gen_acc_short.json \\\n",
    "    --repo-id your-username/dataset-name \\\n",
    "    --token your-huggingface-token"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
