from dataclasses import dataclass
import typing as T
import torch
import functools
import httpx
import transformers
from openai import OpenAI
from anthropic import Anthropic


@dataclass
class LLMOutput:
    """
    Represents the output of a language model.

    Attributes:
        answer (str): The generated text answer from the language model.
        logprobs (torch.Tensor | None): Optional tensor containing the log probabilities
            associated with the generated tokens.
    """
    answer: str
    logprobs: torch.Tensor | None = None  # list of logprobs


@dataclass
class LLMSamples:
    """
    Contains multiple samples generated by a language model along with metadata.

    Attributes:
        samples (List[LLMOutput]): A list of multiple LLMOutput samples.
        answer (LLMOutput): The selected or final answer output.
        params (Dict[str, Any]): Parameters used to generate the samples.
    """
    samples: T.List[LLMOutput]
    answer: LLMOutput
    params: T.Dict[str, T.Any]

    def __len__(self) -> int:
        """
        Returns the number of samples generated.

        Returns:
            int: The count of samples.
        """
        return len(self.samples)


class LLMWrapper:
    def __call__(self, *args, **kwargs) -> LLMOutput:
        """
        Abstract base wrapper for language model interfaces.

        This class is meant to be subclassed to implement specific LLM calls.
        """
        raise NotImplementedError("__call__ should be implemented for your LLM")


class HFLLMWrapper(LLMWrapper):
    """
    Hugging Face LLM wrapper using a tokenizer and model from the transformers library.
    """
    def __init__(
        self, tokenizer: transformers.AutoTokenizer, model: transformers.PreTrainedModel
    ):
        """
        Initializes the HFLLMWrapper with a tokenizer and model.

        Args:
            tokenizer (transformers.AutoTokenizer): A Hugging Face tokenizer instance.
            model (transformers.PreTrainedModel): A Hugging Face model instance.

        Raises:
            ValueError: If the tokenizer or model is not a valid Hugging Face object.
        """
        if isinstance(tokenizer, transformers.PreTrainedTokenizerBase):
            self.tokenizer = tokenizer
        else:
            raise ValueError("Requires a text generation pipeline from transformers")
        if isinstance(model, transformers.PreTrainedModel):
            self.model = model
        else:
            raise ValueError("Requires a text generation pipeline from transformers")

    def __call__(
        self,
        prompt: str,
        temperature: float = 1.0,
        max_new_tokens=1024,
        *args,
        **kwargs
    ) -> LLMOutput:
        """
        Generates a response from the Hugging Face model.

        Args:
            prompt (str): The prompt to send to the model.
            temperature (float): Sampling temperature.
            max_new_tokens (int): Maximum number of tokens to generate.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            LLMOutput: The generated text and associated log probabilities.
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,
                return_dict_in_generate=True,
                output_scores=True,
                do_sample=True,
                temperature=1.0,
            )
        generated_ids = outputs.sequences[0]
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        token_scores = outputs.scores
        generated_tokens = generated_ids[len(inputs["input_ids"][0]) :]

        logprobs = []
        for i, token_id in enumerate(generated_tokens):
            logits = token_scores[i][0]
            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
            token_logprob = log_probs[token_id].item()
            logprobs.append((self.tokenizer.decode([token_id]), token_logprob))

        # logprobs is a list of pairs (token, logprob)
        logprobs = [el[1] for el in logprobs]
        return LLMOutput(
            answer=generated_text,
            logprobs=torch.tensor(logprobs, device=self.model.device),
        )


def update_base_url(request: httpx.Request, openai_endpoint_url: str) -> None:
    """
    Modifies the base URL of an OpenAI request to route to a custom Azure endpoint.

    Args:
        request (httpx.Request): The HTTPX request object to be modified.
        openai_endpoint_url (str): The Azure OpenAI endpoint path to use.
    """
    if request.url.path == "/chat/completions":
        request.url = request.url.copy_with(path=openai_endpoint_url)


class AzureCustomGPT4Wrapper:
    """
    Wrapper for Azure-hosted GPT-4 model using OpenAI-compatible API.
    """
    def __init__(self, openai_endpoint_url, api_key):
        """
        Initializes the Azure GPT-4 wrapper.

        Args:
            openai_endpoint_url (str): The base URL of the Azure OpenAI endpoint.
            api_key (str): Azure API key for authentication.
        """
        self.openai_endpoint_url = openai_endpoint_url
        self.client = OpenAI(
            base_url=openai_endpoint_url,
            api_key=False,
            default_headers={
                "Ocp-Apim-Subscription-Key": api_key,
            },
            http_client=httpx.Client(
                event_hooks={"request": [functools.partial(update_base_url, openai_endpoint_url=openai_endpoint_url)]}
            ),
        )

    def __call__(self, input: str) -> LLMOutput:
        """
        Generates a response using the Azure-hosted GPT-4 model.

        Args:
            input (str): User input prompt.

        Returns:
            LLMOutput: Generated answer and optional log probabilities.
        """
        kwargs = {
            "model": "no_effect",  # Replace with your actual deployment name
            "logprobs": True,
            "messages": [{"role": "user", "content": input}],
            "top_logprobs": 5,
        }
        response = self.client.chat.completions.create(**kwargs)
        content = response.choices[0].message.content
        token_logprobs = response.choices[0].logprobs.token_logprobs
        if token_logprobs is not None:
            logprobs_tensor = torch.tensor(token_logprobs, dtype=torch.float32)
        else:
            logprobs_tensor = None
        return LLMOutput(answer=content, logprobs=logprobs_tensor)


class ClaudeWrapper:
    """
    Wrapper for Anthropic's Claude models.
    """

    def __init__(self, api_key: str):
        """
        Initializes the ClaudeWrapper.

        Args:
            api_key (str): Anthropic API key.
        """
        self.client = Anthropic(api_key=api_key)

    def __call__(self, prompt: str, model: str = "claude-3-opus-20240229", temperature: float = 1.0, max_tokens: int = 1024) -> LLMOutput:
        """
        Generates a response from Claude with optional parameters.

        Note:
            Claude's API currently does not support returning log probabilities.

        Args:
            prompt (str): Input prompt for Claude.
            model (str): Claude model to use.
            temperature (float): Sampling temperature.
            max_tokens (int): Maximum number of tokens to generate.

        Returns:
            LLMOutput: Generated answer text.
        """
        # Anthropic API does not currently support logprobs in the chat API.
        try:
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
            )

            # Extract text response
            text = response.content[0].text if response.content else ""
            return LLMOutput(answer=text)

        except Exception as e:
            raise RuntimeError(f"Claude API call failed: {e}")


class BatchLLMWrapper:
    """
    Abstract class for batch LLM interfaces that return multiple LLM outputs.
    """
    def __call__(self, *args, **kwargs) -> T.List[LLMOutput]:
        """
        Generate multiple responses from an LLM.

        Returns:
            List[LLMOutput]: List of outputs from the model.

        Raises:
            NotImplementedError: If not implemented in a subclass.
        """
        raise NotImplementedError("__call__ should be implemented for your LLM")


def all_logits_present(samples: T.List[LLMOutput]) -> bool:
    """
    Checks whether all LLM outputs contain log probabilities.

    Args:
        samples (List[LLMOutput]): A list of LLM output samples.

    Returns:
        bool: True if all samples include logprobs, False otherwise.
    """
    return all(sample.logprobs is not None for sample in samples)


def generate_n_samples_and_answer(
    llm: LLMWrapper,
    prompt: str,
    temp_gen: float = 1.0,
    temp_answer: float = 0.1,
    top_p_gen: float = 0.9,
    top_k_gen: float = 16,
    top_p_ans: float = 0.7,
    top_k_ans: float = 4,
    n_samples: int = 10,
) -> LLMSamples:
    """
    Generates multiple LLM samples and a single final answer using specified parameters.

    Args:
        llm (LLMWrapper): The language model wrapper to use.
        prompt (str): The prompt to pass to the LLM.
        temp_gen (float): Temperature for generating samples.
        temp_answer (float): Temperature for the final answer.
        top_p_gen (float): Nucleus sampling parameter for generation.
        top_k_gen (int): Top-k sampling parameter for generation.
        top_p_ans (float): Nucleus sampling parameter for answer.
        top_k_ans (int): Top-k sampling parameter for answer.
        n_samples (int): Number of samples to generate.

    Returns:
        LLMSamples: A collection of generated samples, the final answer, and parameters used.

    Raises:
        NotImplementedError: If `llm` is not an instance of LLMWrapper.
    """
    if isinstance(llm, LLMWrapper):
        sampled_answers = [
            llm(prompt, temperature=temp_gen, top_p=top_p_gen, top_k=top_k_gen)
            for _ in range(n_samples)
        ]
        answer = llm(prompt, temperature=temp_gen, top_p=top_p_gen, top_k=top_k_gen)
        params = {
            "prompt": prompt,
            "temp_gen": temp_gen,
            "temp_answer": temp_answer,
            "top_p_gen": top_p_gen,
            "top_k_gen": top_k_gen,
            "top_p_ans": top_p_ans,
            "top_k_ans": top_k_ans,
            "n_samples": n_samples,
            "llm": str(llm),
        }
        return LLMSamples(samples=sampled_answers, answer=answer, params=params)
    else:
        raise NotImplementedError(
            "generation is currently supported only for LLMWrapper"
        )
