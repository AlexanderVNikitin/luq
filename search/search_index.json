{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Language Models Uncertainty Quantification (LUQ)","text":""},{"location":"#get-started","title":"Get Started","text":""},{"location":"#install-luq","title":"Install LUQ:","text":"<pre><code>pip install luq\n</code></pre>"},{"location":"#use-luq-model-for-uq","title":"Use LUQ model for UQ","text":"<pre><code>import luq\nfrom luq.models import MaxProbabilityEstimator\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n# Create text generation pipeline\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# sample from LLM\nsamples = luq.llm.generate_n_samples_and_answer(\n    pipeline,\n    prompt=\"A, B, C, or D\"\n)\n\nmp_estimator = MaxProbabilityEstimator()\nprint(mp_estimator.estimate_uncertainty(samples))\n</code></pre>"},{"location":"#uncertainty-quantification-methods","title":"Uncertainty Quantification Methods","text":"<p>Generally the uncertainty quantification in LUQ sample multiple responses from an LLM and analyse the</p> Method Class in LUQ Note Reference Max Probability <code>luq.models.max_probability</code> Estimates uncertainty as one minus the probability of the most likely sequence in the list of samples. - Top K Gap <code>luq.models.top_k_gap</code> Estimates uncertainty by measuring the gap between the most probable sequence and the k-th most probable one. - Predictive Entropy <code>luq.models.predictive_entropy</code> Uncertainty is estimated by computing the entropy of probabilities obtained from sampled sequences. https://arxiv.org/pdf/2002.07650 p(true) <code>luq.models.p_true</code> Uncertainty is estimated by computing the entropy of probabilities obtained from sampled sequences. https://arxiv.org/pdf/2002.07650 Semantic Entropy <code>luq.models.semantic_entropy</code> Uncertainty is estimated by performing semantic clustering of LLM responses and calculating the entropy across the clusters. https://arxiv.org/abs/2302.09664 Kernel Language Entropy <code>luq.models.kernel_language_entropy</code> Uncertainty is estimated by performing semantic clustering of LLM responses and calculating the entropy across the clusters. https://arxiv.org/abs/2405.20003"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#use-pre-commit","title":"Use pre-commit","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"#pipeline-for-dataset-creation","title":"Pipeline for dataset creation","text":""},{"location":"#step-1-create-a-processed-version-of-a-dataset","title":"Step 1. Create a processed version of a dataset.","text":"<pre><code>mkdir data/coqa\npython scripts/process_datasets.py \\\n    --dataset=coqa \\\n    --output=data/coqa/processed.json\n</code></pre> <pre><code>import json\n\ndata = json.load(open(\"data/coqa/processed.json\", \"r\"))\nnew_data = {\"train\": data[\"train\"][:2], \"validation\": data[\"validation\"][:2]}\njson.dump(new_data, open(\"data/coqa/processed_short.json\", \"w\"))\n</code></pre>"},{"location":"#step-2-generate-answers-from-llms-and-augment-the-dataset-with-the-dataset","title":"Step 2. Generate answers from LLMs and augment the dataset with the dataset.","text":"<pre><code>python scripts/add_generations_to_dataset.py \\\n    --input-file=./data/coqa/processed_short.json\\\n    --output-file=./data/coqa/processed_gen_short.json\\\n</code></pre>"},{"location":"#step-3-check-accuracy-of-the-answers-given","title":"Step 3. Check accuracy of the answers given","text":"<pre><code>python scripts/eval_accuracy.py \\\n    --input-file=data/coqa/processed_gen_short.json \\\n    --output-file=data/coqa/processed_gen_acc_short.json \\\n    --model-name=gpt2 \\\n    --model-type=huggingface\n</code></pre>"},{"location":"#step-4-upload-the-dataset-to-huggingface","title":"Step 4. Upload the dataset to HuggingFace","text":"<pre><code>python scripts/upload_dataset.py \\\n    --path=data/coqa/processed_gen_acc_short.json \\\n    --repo-id your-username/dataset-name \\\n    --token your-huggingface-token\n</code></pre>"},{"location":"#datasets-end-to-end","title":"Datasets end-to-end","text":"<p>In order to generate a dataset: <pre><code>python scripts/gen_dataset.py --input-file=./data/dummy_data/raw_dummy.json --output-file=./output.json\n</code></pre></p> <p>When a dataset is created we can augment it with accuracies checked by another LLM: <pre><code>python scripts/eval_accuracy.py --input-file ./output.json --output-file test.json --model-name=gpt2 --model-type=huggingface\n</code></pre></p>"},{"location":"reference/","title":"LUQ API Documentation","text":""},{"location":"reference/#methods","title":"Methods","text":""},{"location":"reference/#luq.methods.BaseUQModel","title":"<code>BaseUQModel</code>","text":"Source code in <code>luq/methods/base_uq_model.py</code> <pre><code>class BaseUQModel:\n    def compute_sequence_probability(\n        self, logprobs: torch.Tensor, seq_prob_mode: SeqProbMode = SeqProbMode.PROD\n    ) -&gt; float:\n        \"\"\"\n        Computes the probability of a response sequence from log-probabilities.\n\n        Args:\n            logprobs (torch.Tensor): A tensor containing log-probabilities of each token in the sequence.\n            seq_prob_mode (SeqProbMode, optional): The method to compute the sequence probability.\n                Options are SeqProbMode.PROD for product and SeqProbMode.AVG for average.\n                Defaults to SeqProbMode.PROD.\n\n        Returns:\n            float: The computed sequence probability.\n\n        Raises:\n            ValueError: If an unknown `seq_prob_mode` is provided.\n        \"\"\"\n        token_probs = torch.exp(logprobs)  # Convert logits to probabilities\n        if seq_prob_mode == SeqProbMode.PROD:\n            return torch.prod(token_probs).item()\n        elif seq_prob_mode == SeqProbMode.AVG:\n            return torch.mean(token_probs).item()\n        else:\n            raise ValueError(f\"Unknown seq_prob_mode: {seq_prob_mode}\")\n\n    def normalize_sequence_probs(\n        self, probs: List[float], tolerance: float = 1e-9\n    ) -&gt; List[float]:\n        \"\"\"\n        Normalizes a list of sequence probabilities so they sum to 1.\n\n        Args:\n            probs (List[float]): A list of raw sequence probabilities.\n            tolerance (float, optional): A small threshold below which the sum is considered zero\n                to avoid division by zero. Defaults to 1e-9.\n\n        Returns:\n            List[float]: A list of normalized probabilities summing to 1.\n        \"\"\"\n        z = sum(probs)\n        if abs(z) &lt; tolerance:\n            return [1.0 / len(probs)] * len(probs)\n        return [p / z for p in probs]\n\n    def estimate_uncertainty(self, prompt: str, *args, **kwargs) -&gt; float:\n        \"\"\"\n        Estimates the uncertainty for a given prompt.\n\n        Args:\n            prompt (str): The input prompt to estimate uncertainty for.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            float: The estimated uncertainty value.\n\n        Raises:\n            NotImplementedError: This method must be implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"method get_uncertainty is not implemented\")\n</code></pre>"},{"location":"reference/#luq.methods.BaseUQModel.compute_sequence_probability","title":"<code>compute_sequence_probability(logprobs, seq_prob_mode=SeqProbMode.PROD)</code>","text":"<p>Computes the probability of a response sequence from log-probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>logprobs</code> <code>Tensor</code> <p>A tensor containing log-probabilities of each token in the sequence.</p> required <code>seq_prob_mode</code> <code>SeqProbMode</code> <p>The method to compute the sequence probability. Options are SeqProbMode.PROD for product and SeqProbMode.AVG for average. Defaults to SeqProbMode.PROD.</p> <code>PROD</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed sequence probability.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown <code>seq_prob_mode</code> is provided.</p> Source code in <code>luq/methods/base_uq_model.py</code> <pre><code>def compute_sequence_probability(\n    self, logprobs: torch.Tensor, seq_prob_mode: SeqProbMode = SeqProbMode.PROD\n) -&gt; float:\n    \"\"\"\n    Computes the probability of a response sequence from log-probabilities.\n\n    Args:\n        logprobs (torch.Tensor): A tensor containing log-probabilities of each token in the sequence.\n        seq_prob_mode (SeqProbMode, optional): The method to compute the sequence probability.\n            Options are SeqProbMode.PROD for product and SeqProbMode.AVG for average.\n            Defaults to SeqProbMode.PROD.\n\n    Returns:\n        float: The computed sequence probability.\n\n    Raises:\n        ValueError: If an unknown `seq_prob_mode` is provided.\n    \"\"\"\n    token_probs = torch.exp(logprobs)  # Convert logits to probabilities\n    if seq_prob_mode == SeqProbMode.PROD:\n        return torch.prod(token_probs).item()\n    elif seq_prob_mode == SeqProbMode.AVG:\n        return torch.mean(token_probs).item()\n    else:\n        raise ValueError(f\"Unknown seq_prob_mode: {seq_prob_mode}\")\n</code></pre>"},{"location":"reference/#luq.methods.BaseUQModel.estimate_uncertainty","title":"<code>estimate_uncertainty(prompt, *args, **kwargs)</code>","text":"<p>Estimates the uncertainty for a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt to estimate uncertainty for.</p> required <code>*args</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The estimated uncertainty value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in a subclass.</p> Source code in <code>luq/methods/base_uq_model.py</code> <pre><code>def estimate_uncertainty(self, prompt: str, *args, **kwargs) -&gt; float:\n    \"\"\"\n    Estimates the uncertainty for a given prompt.\n\n    Args:\n        prompt (str): The input prompt to estimate uncertainty for.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        float: The estimated uncertainty value.\n\n    Raises:\n        NotImplementedError: This method must be implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"method get_uncertainty is not implemented\")\n</code></pre>"},{"location":"reference/#luq.methods.BaseUQModel.normalize_sequence_probs","title":"<code>normalize_sequence_probs(probs, tolerance=1e-09)</code>","text":"<p>Normalizes a list of sequence probabilities so they sum to 1.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>List[float]</code> <p>A list of raw sequence probabilities.</p> required <code>tolerance</code> <code>float</code> <p>A small threshold below which the sum is considered zero to avoid division by zero. Defaults to 1e-9.</p> <code>1e-09</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: A list of normalized probabilities summing to 1.</p> Source code in <code>luq/methods/base_uq_model.py</code> <pre><code>def normalize_sequence_probs(\n    self, probs: List[float], tolerance: float = 1e-9\n) -&gt; List[float]:\n    \"\"\"\n    Normalizes a list of sequence probabilities so they sum to 1.\n\n    Args:\n        probs (List[float]): A list of raw sequence probabilities.\n        tolerance (float, optional): A small threshold below which the sum is considered zero\n            to avoid division by zero. Defaults to 1e-9.\n\n    Returns:\n        List[float]: A list of normalized probabilities summing to 1.\n    \"\"\"\n    z = sum(probs)\n    if abs(z) &lt; tolerance:\n        return [1.0 / len(probs)] * len(probs)\n    return [p / z for p in probs]\n</code></pre>"},{"location":"reference/#luq.methods.KernelLanguageEntropyEstimator","title":"<code>KernelLanguageEntropyEstimator</code>","text":"<p>               Bases: <code>BaseUQModel</code></p> Source code in <code>luq/methods/kernel_language_entropy.py</code> <pre><code>class KernelLanguageEntropyEstimator(BaseUQModel):\n    def __init__(self):\n        \"\"\"Initializes the KernelLanguageEntropyEstimator.\"\"\"\n        super().__init__()\n\n    def compute_entropy(\n        self,\n        kernel: torch.Tensor,\n        normalize: bool = False,\n    ) -&gt; float:\n        \"\"\"Computes the von Neumann entropy of a given unit-trace kernel matrix (semantic kernel matrix).\n\n        Args:\n            kernel (torch.Tensor): The kernel matrix.\n            normalize (bool, optional): If True, normalize the kernel before computing entropy. Defaults to False.\n\n        Returns:\n            float: The computed Kernel Language Entropy.\n        \"\"\"\n        if normalize:\n            kernel = normalize_kernel(kernel)\n        return von_neumann_entropy(kernel)\n\n    def get_kernel(\n        self,\n        samples: LLMSamples,\n        kernel_type: KernelType | None = None,\n        construct_kernel: T.Callable | None = None,\n        nli_model: NLIWrapper | None = None,\n        nli_table: NLITable | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Constructs a kernel matrix from language model samples.\n\n        Either `kernel_type` or `construct_kernel` must be provided, but not both.\n\n        Args:\n            samples (LLMSamples): The language model samples.\n            kernel_type (KernelType | None, optional): The predefined kernel type to use. Defaults to None.\n            construct_kernel (Callable | None, optional): A custom kernel construction function. Defaults to None.\n            nli_model (NLIWrapper | None, optional): A model for natural language inference. Defaults to None.\n            nli_table (NLITable | None, optional): A precomputed NLI similarity table. Defaults to None.\n\n        Returns:\n            torch.Tensor: The normalized kernel matrix.\n\n        Raises:\n            ValueError: If both or neither `kernel_type` and `construct_kernel` are provided.\n            ValueError: If an unknown kernel type is specified.\n        \"\"\"\n        if kernel_type is not None and construct_kernel is not None:\n            raise ValueError(\n                \"Only one of `kernel_type` and `construct_kernel` should be specified\"\n            )\n        if kernel_type is None and construct_kernel is None:\n            raise ValueError(\n                \"Either `kernel_type` or `construct_kernel` should be specified\"\n            )\n\n        if kernel_type is not None:\n            kernel = None\n            if kernel_type == KernelType.HEAT:\n                # todo: calculate heat kernel\n                pass\n            elif kernel_type == KernelType.MATERN:\n                # todo: calculate Matern kernel\n                pass\n            else:\n                raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n        else:\n            kernel = construct_kernel(samples)\n        kernel = normalize_kernel(kernel)\n        return kernel\n\n    def estimate_uncertainty(\n        self,\n        samples: LLMSamples,\n        seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n        kernel_type: KernelType = KernelType.HEAT,\n        nli_model: NLIWrapper | None = None,\n        nli_table: NLITable | None = None,\n        construct_kernel: T.Callable | None = None,\n        **kwargs,\n    ) -&gt; float:\n        \"\"\"Estimates uncertainty by computing the von Neumann entropy of a semantic similarity kernel.\n\n        One of `nli_model` or `nli_table` must be provided to compute the semantic similarity.\n\n        Args:\n            samples (LLMSamples): The language model samples to analyze.\n            seq_prob_mode (SeqProbMode, optional): Mode for sequence probability aggregation. Defaults to SeqProbMode.PROD.\n            kernel_type (KernelType, optional): The predefined kernel type to use if `construct_kernel` is not provided. Defaults to KernelType.HEAT.\n            nli_model (NLIWrapper | None, optional): A model for natural language inference. Defaults to None.\n            nli_table (NLITable | None, optional): A precomputed NLI similarity table. Defaults to None.\n            construct_kernel (Callable | None, optional): A custom kernel construction function. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            float: The estimated uncertainty value.\n\n        Raises:\n            ValueError: If neither or both `nli_model` and `nli_table` are provided.\n        \"\"\"\n        # validation\n        if nli_model is None and nli_table is None:\n            raise ValueError(\"Either `nli_model` or `nli_table` should be provided\")\n\n        if nli_model is not None and nli_table is not None:\n            raise ValueError(\n                \"Only one of `nli_model` and `nli_table` should be provided\"\n            )\n\n        kernel = self.get_kernel(\n            samples,\n            kernel_type=kernel_type,\n            construct_kernel=construct_kernel,\n            nli_model=nli_model,\n            nli_table=nli_table,\n        )\n        # Compute entropy over clusters\n        return self.compute_entropy(kernel)\n</code></pre>"},{"location":"reference/#luq.methods.KernelLanguageEntropyEstimator.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the KernelLanguageEntropyEstimator.</p> Source code in <code>luq/methods/kernel_language_entropy.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the KernelLanguageEntropyEstimator.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/#luq.methods.KernelLanguageEntropyEstimator.compute_entropy","title":"<code>compute_entropy(kernel, normalize=False)</code>","text":"<p>Computes the von Neumann entropy of a given unit-trace kernel matrix (semantic kernel matrix).</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Tensor</code> <p>The kernel matrix.</p> required <code>normalize</code> <code>bool</code> <p>If True, normalize the kernel before computing entropy. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Kernel Language Entropy.</p> Source code in <code>luq/methods/kernel_language_entropy.py</code> <pre><code>def compute_entropy(\n    self,\n    kernel: torch.Tensor,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Computes the von Neumann entropy of a given unit-trace kernel matrix (semantic kernel matrix).\n\n    Args:\n        kernel (torch.Tensor): The kernel matrix.\n        normalize (bool, optional): If True, normalize the kernel before computing entropy. Defaults to False.\n\n    Returns:\n        float: The computed Kernel Language Entropy.\n    \"\"\"\n    if normalize:\n        kernel = normalize_kernel(kernel)\n    return von_neumann_entropy(kernel)\n</code></pre>"},{"location":"reference/#luq.methods.KernelLanguageEntropyEstimator.estimate_uncertainty","title":"<code>estimate_uncertainty(samples, seq_prob_mode=SeqProbMode.PROD, kernel_type=KernelType.HEAT, nli_model=None, nli_table=None, construct_kernel=None, **kwargs)</code>","text":"<p>Estimates uncertainty by computing the von Neumann entropy of a semantic similarity kernel.</p> <p>One of <code>nli_model</code> or <code>nli_table</code> must be provided to compute the semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>LLMSamples</code> <p>The language model samples to analyze.</p> required <code>seq_prob_mode</code> <code>SeqProbMode</code> <p>Mode for sequence probability aggregation. Defaults to SeqProbMode.PROD.</p> <code>PROD</code> <code>kernel_type</code> <code>KernelType</code> <p>The predefined kernel type to use if <code>construct_kernel</code> is not provided. Defaults to KernelType.HEAT.</p> <code>HEAT</code> <code>nli_model</code> <code>NLIWrapper | None</code> <p>A model for natural language inference. Defaults to None.</p> <code>None</code> <code>nli_table</code> <code>NLITable | None</code> <p>A precomputed NLI similarity table. Defaults to None.</p> <code>None</code> <code>construct_kernel</code> <code>Callable | None</code> <p>A custom kernel construction function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The estimated uncertainty value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both <code>nli_model</code> and <code>nli_table</code> are provided.</p> Source code in <code>luq/methods/kernel_language_entropy.py</code> <pre><code>def estimate_uncertainty(\n    self,\n    samples: LLMSamples,\n    seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n    kernel_type: KernelType = KernelType.HEAT,\n    nli_model: NLIWrapper | None = None,\n    nli_table: NLITable | None = None,\n    construct_kernel: T.Callable | None = None,\n    **kwargs,\n) -&gt; float:\n    \"\"\"Estimates uncertainty by computing the von Neumann entropy of a semantic similarity kernel.\n\n    One of `nli_model` or `nli_table` must be provided to compute the semantic similarity.\n\n    Args:\n        samples (LLMSamples): The language model samples to analyze.\n        seq_prob_mode (SeqProbMode, optional): Mode for sequence probability aggregation. Defaults to SeqProbMode.PROD.\n        kernel_type (KernelType, optional): The predefined kernel type to use if `construct_kernel` is not provided. Defaults to KernelType.HEAT.\n        nli_model (NLIWrapper | None, optional): A model for natural language inference. Defaults to None.\n        nli_table (NLITable | None, optional): A precomputed NLI similarity table. Defaults to None.\n        construct_kernel (Callable | None, optional): A custom kernel construction function. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        float: The estimated uncertainty value.\n\n    Raises:\n        ValueError: If neither or both `nli_model` and `nli_table` are provided.\n    \"\"\"\n    # validation\n    if nli_model is None and nli_table is None:\n        raise ValueError(\"Either `nli_model` or `nli_table` should be provided\")\n\n    if nli_model is not None and nli_table is not None:\n        raise ValueError(\n            \"Only one of `nli_model` and `nli_table` should be provided\"\n        )\n\n    kernel = self.get_kernel(\n        samples,\n        kernel_type=kernel_type,\n        construct_kernel=construct_kernel,\n        nli_model=nli_model,\n        nli_table=nli_table,\n    )\n    # Compute entropy over clusters\n    return self.compute_entropy(kernel)\n</code></pre>"},{"location":"reference/#luq.methods.KernelLanguageEntropyEstimator.get_kernel","title":"<code>get_kernel(samples, kernel_type=None, construct_kernel=None, nli_model=None, nli_table=None)</code>","text":"<p>Constructs a kernel matrix from language model samples.</p> <p>Either <code>kernel_type</code> or <code>construct_kernel</code> must be provided, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>LLMSamples</code> <p>The language model samples.</p> required <code>kernel_type</code> <code>KernelType | None</code> <p>The predefined kernel type to use. Defaults to None.</p> <code>None</code> <code>construct_kernel</code> <code>Callable | None</code> <p>A custom kernel construction function. Defaults to None.</p> <code>None</code> <code>nli_model</code> <code>NLIWrapper | None</code> <p>A model for natural language inference. Defaults to None.</p> <code>None</code> <code>nli_table</code> <code>NLITable | None</code> <p>A precomputed NLI similarity table. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The normalized kernel matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both or neither <code>kernel_type</code> and <code>construct_kernel</code> are provided.</p> <code>ValueError</code> <p>If an unknown kernel type is specified.</p> Source code in <code>luq/methods/kernel_language_entropy.py</code> <pre><code>def get_kernel(\n    self,\n    samples: LLMSamples,\n    kernel_type: KernelType | None = None,\n    construct_kernel: T.Callable | None = None,\n    nli_model: NLIWrapper | None = None,\n    nli_table: NLITable | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Constructs a kernel matrix from language model samples.\n\n    Either `kernel_type` or `construct_kernel` must be provided, but not both.\n\n    Args:\n        samples (LLMSamples): The language model samples.\n        kernel_type (KernelType | None, optional): The predefined kernel type to use. Defaults to None.\n        construct_kernel (Callable | None, optional): A custom kernel construction function. Defaults to None.\n        nli_model (NLIWrapper | None, optional): A model for natural language inference. Defaults to None.\n        nli_table (NLITable | None, optional): A precomputed NLI similarity table. Defaults to None.\n\n    Returns:\n        torch.Tensor: The normalized kernel matrix.\n\n    Raises:\n        ValueError: If both or neither `kernel_type` and `construct_kernel` are provided.\n        ValueError: If an unknown kernel type is specified.\n    \"\"\"\n    if kernel_type is not None and construct_kernel is not None:\n        raise ValueError(\n            \"Only one of `kernel_type` and `construct_kernel` should be specified\"\n        )\n    if kernel_type is None and construct_kernel is None:\n        raise ValueError(\n            \"Either `kernel_type` or `construct_kernel` should be specified\"\n        )\n\n    if kernel_type is not None:\n        kernel = None\n        if kernel_type == KernelType.HEAT:\n            # todo: calculate heat kernel\n            pass\n        elif kernel_type == KernelType.MATERN:\n            # todo: calculate Matern kernel\n            pass\n        else:\n            raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n    else:\n        kernel = construct_kernel(samples)\n    kernel = normalize_kernel(kernel)\n    return kernel\n</code></pre>"},{"location":"reference/#luq.methods.KernelType","title":"<code>KernelType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of supported kernel types.</p> <p>Attributes:</p> Name Type Description <code>HEAT</code> <code>str</code> <p>Heat kernel type.</p> <code>MATERN</code> <code>str</code> <p>Matern kernel type.</p> Source code in <code>luq/methods/kernel_utils.py</code> <pre><code>class KernelType(Enum):\n    \"\"\"Enumeration of supported kernel types.\n\n    Attributes:\n        HEAT (str): Heat kernel type.\n        MATERN (str): Matern kernel type.\n    \"\"\"\n    HEAT: str = \"heat\"\n    MATERN: str = \"matern\"\n</code></pre>"},{"location":"reference/#luq.methods.LLMOutput","title":"<code>LLMOutput</code>  <code>dataclass</code>","text":"<p>Represents the output of a language model.</p> <p>Attributes:</p> Name Type Description <code>answer</code> <code>str</code> <p>The generated text answer from the language model.</p> <code>logprobs</code> <code>Tensor | None</code> <p>Optional tensor containing the log probabilities associated with the generated tokens.</p> Source code in <code>luq/models/llm.py</code> <pre><code>@dataclass\nclass LLMOutput:\n    \"\"\"\n    Represents the output of a language model.\n\n    Attributes:\n        answer (str): The generated text answer from the language model.\n        logprobs (torch.Tensor | None): Optional tensor containing the log probabilities\n            associated with the generated tokens.\n    \"\"\"\n    answer: str\n    logprobs: torch.Tensor | None = None  # list of logprobs\n</code></pre>"},{"location":"reference/#luq.methods.LLMSamples","title":"<code>LLMSamples</code>  <code>dataclass</code>","text":"<p>Contains multiple samples generated by a language model along with metadata.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <code>List[LLMOutput]</code> <p>A list of multiple LLMOutput samples.</p> <code>answer</code> <code>LLMOutput</code> <p>The selected or final answer output.</p> <code>params</code> <code>Dict[str, Any]</code> <p>Parameters used to generate the samples.</p> Source code in <code>luq/models/llm.py</code> <pre><code>@dataclass\nclass LLMSamples:\n    \"\"\"\n    Contains multiple samples generated by a language model along with metadata.\n\n    Attributes:\n        samples (List[LLMOutput]): A list of multiple LLMOutput samples.\n        answer (LLMOutput): The selected or final answer output.\n        params (Dict[str, Any]): Parameters used to generate the samples.\n    \"\"\"\n    samples: T.List[LLMOutput]\n    answer: LLMOutput\n    params: T.Dict[str, T.Any]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of samples generated.\n\n        Returns:\n            int: The count of samples.\n        \"\"\"\n        return len(self.samples)\n</code></pre>"},{"location":"reference/#luq.methods.LLMSamples.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples generated.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of samples.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of samples generated.\n\n    Returns:\n        int: The count of samples.\n    \"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"reference/#luq.methods.LLMWrapper","title":"<code>LLMWrapper</code>","text":"Source code in <code>luq/models/llm.py</code> <pre><code>class LLMWrapper:\n    def __call__(self, *args, **kwargs) -&gt; LLMOutput:\n        \"\"\"\n        Abstract base wrapper for language model interfaces.\n\n        This class is meant to be subclassed to implement specific LLM calls.\n        \"\"\"\n        raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.methods.LLMWrapper.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Abstract base wrapper for language model interfaces.</p> <p>This class is meant to be subclassed to implement specific LLM calls.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; LLMOutput:\n    \"\"\"\n    Abstract base wrapper for language model interfaces.\n\n    This class is meant to be subclassed to implement specific LLM calls.\n    \"\"\"\n    raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.methods.MaxProbabilityEstimator","title":"<code>MaxProbabilityEstimator</code>","text":"<p>               Bases: <code>BaseUQModel</code></p> <p>Uncertainty estimator that uses the probability of the most likely sequence.</p> <p>This class estimates uncertainty by computing the probability of each sequence in a set of samples, and returning one minus the maximum probability, which serves as a measure of uncertainty.</p> Source code in <code>luq/methods/max_probability.py</code> <pre><code>class MaxProbabilityEstimator(BaseUQModel):\n    \"\"\"Uncertainty estimator that uses the probability of the most likely sequence.\n\n    This class estimates uncertainty by computing the probability of each sequence in a set of samples,\n    and returning one minus the maximum probability, which serves as a measure of uncertainty.\n    \"\"\"\n    def estimate_uncertainty(\n        self,\n        samples: T.List[LLMOutput],\n        seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n        **kwargs,\n    ) -&gt; float:\n        \"\"\"Estimate uncertainty from a list of LLM output samples.\n\n        This method calculates the sequence probability for each sample using the specified\n        sequence probability mode and returns an uncertainty score equal to `1 - max(sequence_probs)`.\n\n        Args:\n            samples (List[LLMOutput]): A list of language model outputs with associated log probabilities.\n            seq_prob_mode (SeqProbMode, optional): Mode for aggregating token probabilities into\n                sequence probabilities (e.g., product or average). Defaults to `SeqProbMode.PROD`.\n            **kwargs: Additional keyword arguments (unused here but kept for compatibility).\n\n        Returns:\n            float: Uncertainty score, where higher values indicate more uncertainty.\n        \"\"\"\n        assert all(s.logprobs is not None for s in samples.samples)\n\n        logit_samples = [s.logprobs for s in samples.samples]\n        sequence_probs = [\n            self.compute_sequence_probability(logits, seq_prob_mode)\n            for logits in logit_samples\n        ]\n        sequence_probs = self.normalize_sequence_probs(sequence_probs)\n        return 1 - max(sequence_probs)\n</code></pre>"},{"location":"reference/#luq.methods.MaxProbabilityEstimator.estimate_uncertainty","title":"<code>estimate_uncertainty(samples, seq_prob_mode=SeqProbMode.PROD, **kwargs)</code>","text":"<p>Estimate uncertainty from a list of LLM output samples.</p> <p>This method calculates the sequence probability for each sample using the specified sequence probability mode and returns an uncertainty score equal to <code>1 - max(sequence_probs)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[LLMOutput]</code> <p>A list of language model outputs with associated log probabilities.</p> required <code>seq_prob_mode</code> <code>SeqProbMode</code> <p>Mode for aggregating token probabilities into sequence probabilities (e.g., product or average). Defaults to <code>SeqProbMode.PROD</code>.</p> <code>PROD</code> <code>**kwargs</code> <p>Additional keyword arguments (unused here but kept for compatibility).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Uncertainty score, where higher values indicate more uncertainty.</p> Source code in <code>luq/methods/max_probability.py</code> <pre><code>def estimate_uncertainty(\n    self,\n    samples: T.List[LLMOutput],\n    seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n    **kwargs,\n) -&gt; float:\n    \"\"\"Estimate uncertainty from a list of LLM output samples.\n\n    This method calculates the sequence probability for each sample using the specified\n    sequence probability mode and returns an uncertainty score equal to `1 - max(sequence_probs)`.\n\n    Args:\n        samples (List[LLMOutput]): A list of language model outputs with associated log probabilities.\n        seq_prob_mode (SeqProbMode, optional): Mode for aggregating token probabilities into\n            sequence probabilities (e.g., product or average). Defaults to `SeqProbMode.PROD`.\n        **kwargs: Additional keyword arguments (unused here but kept for compatibility).\n\n    Returns:\n        float: Uncertainty score, where higher values indicate more uncertainty.\n    \"\"\"\n    assert all(s.logprobs is not None for s in samples.samples)\n\n    logit_samples = [s.logprobs for s in samples.samples]\n    sequence_probs = [\n        self.compute_sequence_probability(logits, seq_prob_mode)\n        for logits in logit_samples\n    ]\n    sequence_probs = self.normalize_sequence_probs(sequence_probs)\n    return 1 - max(sequence_probs)\n</code></pre>"},{"location":"reference/#luq.methods.NLIWrapper","title":"<code>NLIWrapper</code>","text":"<p>Abstract wrapper class for Natural Language Inference (NLI) models.</p> Source code in <code>luq/models/nli.py</code> <pre><code>class NLIWrapper:\n    \"\"\"\n    Abstract wrapper class for Natural Language Inference (NLI) models.\n    \"\"\"\n    def __call__(*args, **kwargs) -&gt; T.List[NLIOutput]:\n        \"\"\"\n        Runs the NLI model on input arguments.\n\n        Returns:\n            List[NLIOutput]: A list of NLI model outputs.\n\n        Raises:\n            NotImplementedError: If not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"NLI model should implement `__call__` method.\")\n</code></pre>"},{"location":"reference/#luq.methods.NLIWrapper.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Runs the NLI model on input arguments.</p> <p>Returns:</p> Type Description <code>List[NLIOutput]</code> <p>List[NLIOutput]: A list of NLI model outputs.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented in a subclass.</p> Source code in <code>luq/models/nli.py</code> <pre><code>def __call__(*args, **kwargs) -&gt; T.List[NLIOutput]:\n    \"\"\"\n    Runs the NLI model on input arguments.\n\n    Returns:\n        List[NLIOutput]: A list of NLI model outputs.\n\n    Raises:\n        NotImplementedError: If not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"NLI model should implement `__call__` method.\")\n</code></pre>"},{"location":"reference/#luq.methods.PredictiveEntropyEstimator","title":"<code>PredictiveEntropyEstimator</code>","text":"<p>               Bases: <code>BaseUQModel</code></p> Source code in <code>luq/methods/predictive_entropy.py</code> <pre><code>class PredictiveEntropyEstimator(BaseUQModel):\n    def generate_logits(self, prompt: str, num_samples: int = 10) -&gt; T.List:\n        \"\"\"Generates multiple responses from the language model and extracts their logits.\n\n        Args:\n            prompt (str): The input prompt for the language model.\n            num_samples (int, optional): Number of samples to generate. Defaults to 10.\n\n        Returns:\n            List: A list of logit sequences corresponding to the generated samples.\n\n        Raises:\n            ValueError: If the internal language model is not an instance of LLMWrapper.\n        \"\"\"\n        logit_samples = []\n\n        for _ in range(num_samples):\n            if isinstance(self._llm, LLMWrapper):\n                response = self._llm(prompt)\n            else:\n                raise ValueError(\n                    f\"Cannot compute logits LogitUncertaintyQuantification for {type(self._llm)}\"\n                )\n            logit_samples.append(response.logits)\n\n        return logit_samples\n\n    def compute_entropy(self, sequence_probs: torch.Tensor | List) -&gt; float:\n        \"\"\"Computes the entropy over a list of sequence probabilities.\n\n        Args:\n            sequence_probs (list or torch.Tensor): List or tensor of sequence probabilities.\n\n        Returns:\n            float: The entropy value computed from the normalized probability distribution.\n        \"\"\"\n        if not isinstance(sequence_probs, torch.Tensor):\n            sequence_probs = torch.tensor(sequence_probs)\n\n        sequence_probs /= sum(\n            sequence_probs\n        )  # Normalize to form a probability distribution\n        return entropy(sequence_probs)\n\n    def estimate_uncertainty(\n        self,\n        samples: T.List[LLMOutput],\n        seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n        **kwargs,\n    ) -&gt; float:\n        \"\"\"\n        Uncertainty is estimated by computing the entropy of probabilities obtained from sampled sequences.\n\n        :param prompt: The input prompt for LLM.\n        :param seq_prob_mode: Describes how token probabilities are translated into sequence probabilities\n        :return: entropy score\n        \"\"\"\n        assert all(s.logprobs is not None for s in samples.samples)\n\n        logit_samples = [s.logprobs for s in samples.samples]\n        sequence_probs = [\n            self.compute_sequence_probability(logits, seq_prob_mode)\n            for logits in logit_samples\n        ]\n        entropy_value = self.compute_entropy(sequence_probs)\n\n        return entropy_value\n</code></pre>"},{"location":"reference/#luq.methods.PredictiveEntropyEstimator.compute_entropy","title":"<code>compute_entropy(sequence_probs)</code>","text":"<p>Computes the entropy over a list of sequence probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_probs</code> <code>list or Tensor</code> <p>List or tensor of sequence probabilities.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The entropy value computed from the normalized probability distribution.</p> Source code in <code>luq/methods/predictive_entropy.py</code> <pre><code>def compute_entropy(self, sequence_probs: torch.Tensor | List) -&gt; float:\n    \"\"\"Computes the entropy over a list of sequence probabilities.\n\n    Args:\n        sequence_probs (list or torch.Tensor): List or tensor of sequence probabilities.\n\n    Returns:\n        float: The entropy value computed from the normalized probability distribution.\n    \"\"\"\n    if not isinstance(sequence_probs, torch.Tensor):\n        sequence_probs = torch.tensor(sequence_probs)\n\n    sequence_probs /= sum(\n        sequence_probs\n    )  # Normalize to form a probability distribution\n    return entropy(sequence_probs)\n</code></pre>"},{"location":"reference/#luq.methods.PredictiveEntropyEstimator.estimate_uncertainty","title":"<code>estimate_uncertainty(samples, seq_prob_mode=SeqProbMode.PROD, **kwargs)</code>","text":"<p>Uncertainty is estimated by computing the entropy of probabilities obtained from sampled sequences.</p> <p>:param prompt: The input prompt for LLM. :param seq_prob_mode: Describes how token probabilities are translated into sequence probabilities :return: entropy score</p> Source code in <code>luq/methods/predictive_entropy.py</code> <pre><code>def estimate_uncertainty(\n    self,\n    samples: T.List[LLMOutput],\n    seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n    **kwargs,\n) -&gt; float:\n    \"\"\"\n    Uncertainty is estimated by computing the entropy of probabilities obtained from sampled sequences.\n\n    :param prompt: The input prompt for LLM.\n    :param seq_prob_mode: Describes how token probabilities are translated into sequence probabilities\n    :return: entropy score\n    \"\"\"\n    assert all(s.logprobs is not None for s in samples.samples)\n\n    logit_samples = [s.logprobs for s in samples.samples]\n    sequence_probs = [\n        self.compute_sequence_probability(logits, seq_prob_mode)\n        for logits in logit_samples\n    ]\n    entropy_value = self.compute_entropy(sequence_probs)\n\n    return entropy_value\n</code></pre>"},{"location":"reference/#luq.methods.PredictiveEntropyEstimator.generate_logits","title":"<code>generate_logits(prompt, num_samples=10)</code>","text":"<p>Generates multiple responses from the language model and extracts their logits.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for the language model.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A list of logit sequences corresponding to the generated samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the internal language model is not an instance of LLMWrapper.</p> Source code in <code>luq/methods/predictive_entropy.py</code> <pre><code>def generate_logits(self, prompt: str, num_samples: int = 10) -&gt; T.List:\n    \"\"\"Generates multiple responses from the language model and extracts their logits.\n\n    Args:\n        prompt (str): The input prompt for the language model.\n        num_samples (int, optional): Number of samples to generate. Defaults to 10.\n\n    Returns:\n        List: A list of logit sequences corresponding to the generated samples.\n\n    Raises:\n        ValueError: If the internal language model is not an instance of LLMWrapper.\n    \"\"\"\n    logit_samples = []\n\n    for _ in range(num_samples):\n        if isinstance(self._llm, LLMWrapper):\n            response = self._llm(prompt)\n        else:\n            raise ValueError(\n                f\"Cannot compute logits LogitUncertaintyQuantification for {type(self._llm)}\"\n            )\n        logit_samples.append(response.logits)\n\n    return logit_samples\n</code></pre>"},{"location":"reference/#luq.methods.SemanticEntropyEstimator","title":"<code>SemanticEntropyEstimator</code>","text":"<p>               Bases: <code>BaseUQModel</code></p> Source code in <code>luq/methods/semantic_entropy.py</code> <pre><code>class SemanticEntropyEstimator(BaseUQModel):\n    def __init__(self):\n        \"\"\"Initializes the SemanticEntropyEstimator.\"\"\"\n        super().__init__()\n\n    def compute_entropy(\n        self, cluster_assignments: T.List[int], sequence_probs: T.List[float] | None\n    ) -&gt; float:\n        \"\"\"Computes entropy over semantic clusters.\n\n        Entropy is calculated either using:\n        - Cluster sizes (discrete entropy), or\n        - Weighted sequence probabilities assigned to clusters (continuous entropy).\n\n        Args:\n            cluster_assignments (List[int]): List mapping each response to a cluster ID.\n            sequence_probs (List[float] | None): List of sequence probabilities. If None,\n                discrete entropy is computed based on cluster sizes.\n\n        Returns:\n            float: Entropy value representing semantic uncertainty.\n        \"\"\"\n        if sequence_probs is None:\n            # Discrete Semantic Entropy\n            cluster_counts = Counter(cluster_assignments)\n            cluster_probs = torch.tensor(\n                [\n                    count / sum(cluster_counts.values())\n                    for count in cluster_counts.values()\n                ]\n            )\n        else:\n            # Continuous Semantic Entropy with sequence probabilities\n            cluster_probs = torch.zeros(max(cluster_assignments) + 1)\n            for cluster_id, prob in zip(cluster_assignments, sequence_probs):\n                cluster_probs[cluster_id] += prob\n            # Normalize probabilities\n            cluster_probs = cluster_probs / torch.sum(cluster_probs)\n\n        return entropy(cluster_probs)\n\n    def estimate_uncertainty(\n        self,\n        samples: LLMSamples,\n        seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n        nli_model: NLIWrapper | None = None,\n        nli_table: NLITable | None = None,\n        **kwargs,\n    ) -&gt; float:\n        \"\"\"Estimates uncertainty based on the semantic diversity of LLM responses.\n\n        Semantic uncertainty is computed by clustering responses into meaning-based groups\n        using an NLI model or precomputed NLI table, and then calculating entropy across\n        these clusters.\n\n        Args:\n            samples (LLMSamples): List of LLM responses containing text and log-probabilities.\n            seq_prob_mode (SeqProbMode, optional): Defines how to compute sequence probabilities\n                from token log-probabilities. Defaults to SeqProbMode.PROD.\n            nli_model (NLIWrapper | None, optional): NLI model used to compute entailment-based similarity.\n            nli_table (NLITable | None, optional): Precomputed NLI similarity table to avoid recomputation.\n            **kwargs: Additional arguments for future extensibility.\n\n        Returns:\n            float: Estimated entropy based on semantic clustering.\n\n        Raises:\n            ValueError: If neither or both of `nli_model` and `nli_table` are provided.\n        \"\"\"\n\n        # validation\n        if nli_model is None and nli_table is None:\n            raise ValueError(\"Either `nli_model` or `nli_table` should be provided\")\n\n        if nli_model is not None and nli_table is not None:\n            raise ValueError(\n                \"Only one of `nli_model` and `nli_table` should be provided\"\n            )\n\n        logit_samples = [s.logprobs for s in samples.samples]\n\n        # Compute sequence probabilities\n        sequence_probs = [\n            self.compute_sequence_probability(logits, seq_prob_mode)\n            for logits in logit_samples\n        ]\n\n        if nli_table is None:\n            nli_table = construct_nli_table(samples, nli_model)\n\n        # Cluster responses\n        cluster_assignments = hard_nli_clustering(samples, nli_table)\n\n        # Compute entropy over clusters\n        return self.compute_entropy(cluster_assignments, sequence_probs)\n</code></pre>"},{"location":"reference/#luq.methods.SemanticEntropyEstimator.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the SemanticEntropyEstimator.</p> Source code in <code>luq/methods/semantic_entropy.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the SemanticEntropyEstimator.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/#luq.methods.SemanticEntropyEstimator.compute_entropy","title":"<code>compute_entropy(cluster_assignments, sequence_probs)</code>","text":"<p>Computes entropy over semantic clusters.</p> <p>Entropy is calculated either using: - Cluster sizes (discrete entropy), or - Weighted sequence probabilities assigned to clusters (continuous entropy).</p> <p>Parameters:</p> Name Type Description Default <code>cluster_assignments</code> <code>List[int]</code> <p>List mapping each response to a cluster ID.</p> required <code>sequence_probs</code> <code>List[float] | None</code> <p>List of sequence probabilities. If None, discrete entropy is computed based on cluster sizes.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Entropy value representing semantic uncertainty.</p> Source code in <code>luq/methods/semantic_entropy.py</code> <pre><code>def compute_entropy(\n    self, cluster_assignments: T.List[int], sequence_probs: T.List[float] | None\n) -&gt; float:\n    \"\"\"Computes entropy over semantic clusters.\n\n    Entropy is calculated either using:\n    - Cluster sizes (discrete entropy), or\n    - Weighted sequence probabilities assigned to clusters (continuous entropy).\n\n    Args:\n        cluster_assignments (List[int]): List mapping each response to a cluster ID.\n        sequence_probs (List[float] | None): List of sequence probabilities. If None,\n            discrete entropy is computed based on cluster sizes.\n\n    Returns:\n        float: Entropy value representing semantic uncertainty.\n    \"\"\"\n    if sequence_probs is None:\n        # Discrete Semantic Entropy\n        cluster_counts = Counter(cluster_assignments)\n        cluster_probs = torch.tensor(\n            [\n                count / sum(cluster_counts.values())\n                for count in cluster_counts.values()\n            ]\n        )\n    else:\n        # Continuous Semantic Entropy with sequence probabilities\n        cluster_probs = torch.zeros(max(cluster_assignments) + 1)\n        for cluster_id, prob in zip(cluster_assignments, sequence_probs):\n            cluster_probs[cluster_id] += prob\n        # Normalize probabilities\n        cluster_probs = cluster_probs / torch.sum(cluster_probs)\n\n    return entropy(cluster_probs)\n</code></pre>"},{"location":"reference/#luq.methods.SemanticEntropyEstimator.estimate_uncertainty","title":"<code>estimate_uncertainty(samples, seq_prob_mode=SeqProbMode.PROD, nli_model=None, nli_table=None, **kwargs)</code>","text":"<p>Estimates uncertainty based on the semantic diversity of LLM responses.</p> <p>Semantic uncertainty is computed by clustering responses into meaning-based groups using an NLI model or precomputed NLI table, and then calculating entropy across these clusters.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>LLMSamples</code> <p>List of LLM responses containing text and log-probabilities.</p> required <code>seq_prob_mode</code> <code>SeqProbMode</code> <p>Defines how to compute sequence probabilities from token log-probabilities. Defaults to SeqProbMode.PROD.</p> <code>PROD</code> <code>nli_model</code> <code>NLIWrapper | None</code> <p>NLI model used to compute entailment-based similarity.</p> <code>None</code> <code>nli_table</code> <code>NLITable | None</code> <p>Precomputed NLI similarity table to avoid recomputation.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for future extensibility.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Estimated entropy based on semantic clustering.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of <code>nli_model</code> and <code>nli_table</code> are provided.</p> Source code in <code>luq/methods/semantic_entropy.py</code> <pre><code>def estimate_uncertainty(\n    self,\n    samples: LLMSamples,\n    seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n    nli_model: NLIWrapper | None = None,\n    nli_table: NLITable | None = None,\n    **kwargs,\n) -&gt; float:\n    \"\"\"Estimates uncertainty based on the semantic diversity of LLM responses.\n\n    Semantic uncertainty is computed by clustering responses into meaning-based groups\n    using an NLI model or precomputed NLI table, and then calculating entropy across\n    these clusters.\n\n    Args:\n        samples (LLMSamples): List of LLM responses containing text and log-probabilities.\n        seq_prob_mode (SeqProbMode, optional): Defines how to compute sequence probabilities\n            from token log-probabilities. Defaults to SeqProbMode.PROD.\n        nli_model (NLIWrapper | None, optional): NLI model used to compute entailment-based similarity.\n        nli_table (NLITable | None, optional): Precomputed NLI similarity table to avoid recomputation.\n        **kwargs: Additional arguments for future extensibility.\n\n    Returns:\n        float: Estimated entropy based on semantic clustering.\n\n    Raises:\n        ValueError: If neither or both of `nli_model` and `nli_table` are provided.\n    \"\"\"\n\n    # validation\n    if nli_model is None and nli_table is None:\n        raise ValueError(\"Either `nli_model` or `nli_table` should be provided\")\n\n    if nli_model is not None and nli_table is not None:\n        raise ValueError(\n            \"Only one of `nli_model` and `nli_table` should be provided\"\n        )\n\n    logit_samples = [s.logprobs for s in samples.samples]\n\n    # Compute sequence probabilities\n    sequence_probs = [\n        self.compute_sequence_probability(logits, seq_prob_mode)\n        for logits in logit_samples\n    ]\n\n    if nli_table is None:\n        nli_table = construct_nli_table(samples, nli_model)\n\n    # Cluster responses\n    cluster_assignments = hard_nli_clustering(samples, nli_table)\n\n    # Compute entropy over clusters\n    return self.compute_entropy(cluster_assignments, sequence_probs)\n</code></pre>"},{"location":"reference/#luq.methods.SeqProbMode","title":"<code>SeqProbMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for modes of combining token probabilities in a sequence.</p> <p>Attributes:</p> Name Type Description <code>PROD</code> <p>Use the product of probabilities.</p> <code>AVG</code> <p>Use the average of probabilities.</p> Source code in <code>luq/utils/utils.py</code> <pre><code>class SeqProbMode(Enum):\n    \"\"\"\n    Enumeration for modes of combining token probabilities in a sequence.\n\n    Attributes:\n        PROD: Use the product of probabilities.\n        AVG: Use the average of probabilities.\n    \"\"\"\n    PROD = \"prod\"\n    AVG = \"avg\"\n</code></pre>"},{"location":"reference/#luq.methods.TopKGapEstimator","title":"<code>TopKGapEstimator</code>","text":"<p>               Bases: <code>BaseUQModel</code></p> Source code in <code>luq/methods/top_k_gap.py</code> <pre><code>class TopKGapEstimator(BaseUQModel):\n    def estimate_uncertainty(\n        self,\n        samples: T.List[LLMOutput],\n        seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n        k: int = 2,\n        **kwargs,\n    ) -&gt; float:\n        \"\"\"Estimates uncertainty using the gap between the top-k sequence probabilities.\n\n        The method computes sequence-level probabilities from the sampled responses,\n        identifies the `k` highest probabilities, and returns a normalized uncertainty\n        score as `1 - (gap between top-1 and top-k probabilities)`.\n\n        A smaller gap between top-k and top-1 implies higher uncertainty (less confident top choice),\n        while a large gap suggests stronger model confidence.\n\n        Args:\n            samples (List[LLMOutput]): A list of LLM outputs containing log-probabilities.\n            seq_prob_mode (SeqProbMode, optional): Method for combining token log-probabilities into\n                a single sequence probability. Defaults to `SeqProbMode.PROD`.\n            k (int, optional): The number of top probabilities to compare. Must be &gt;= 2. Defaults to 2.\n            **kwargs: Additional arguments for extensibility (unused).\n\n        Returns:\n            float: A normalized uncertainty score based on the gap between top-1 and top-k probabilities.\n\n        Raises:\n            ValueError: If `k` is less than 2.\n            AssertionError: If any sample does not contain log-probabilities.\n        \"\"\"\n        if k &lt; 2:\n            raise ValueError(\"k should &gt;= 2\")\n        assert all(s.logprobs is not None for s in samples.samples)\n\n        logit_samples = [s.logprobs for s in samples.samples]\n        sequence_probs = [\n            self.compute_sequence_probability(logits, seq_prob_mode)\n            for logits in logit_samples\n        ]\n        sorted_seq_probs = sorted(sequence_probs)\n        gap = sorted_seq_probs[-1] - sorted_seq_probs[-k]\n        return 1 - gap\n</code></pre>"},{"location":"reference/#luq.methods.TopKGapEstimator.estimate_uncertainty","title":"<code>estimate_uncertainty(samples, seq_prob_mode=SeqProbMode.PROD, k=2, **kwargs)</code>","text":"<p>Estimates uncertainty using the gap between the top-k sequence probabilities.</p> <p>The method computes sequence-level probabilities from the sampled responses, identifies the <code>k</code> highest probabilities, and returns a normalized uncertainty score as <code>1 - (gap between top-1 and top-k probabilities)</code>.</p> <p>A smaller gap between top-k and top-1 implies higher uncertainty (less confident top choice), while a large gap suggests stronger model confidence.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[LLMOutput]</code> <p>A list of LLM outputs containing log-probabilities.</p> required <code>seq_prob_mode</code> <code>SeqProbMode</code> <p>Method for combining token log-probabilities into a single sequence probability. Defaults to <code>SeqProbMode.PROD</code>.</p> <code>PROD</code> <code>k</code> <code>int</code> <p>The number of top probabilities to compare. Must be &gt;= 2. Defaults to 2.</p> <code>2</code> <code>**kwargs</code> <p>Additional arguments for extensibility (unused).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A normalized uncertainty score based on the gap between top-1 and top-k probabilities.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>k</code> is less than 2.</p> <code>AssertionError</code> <p>If any sample does not contain log-probabilities.</p> Source code in <code>luq/methods/top_k_gap.py</code> <pre><code>def estimate_uncertainty(\n    self,\n    samples: T.List[LLMOutput],\n    seq_prob_mode: SeqProbMode = SeqProbMode.PROD,\n    k: int = 2,\n    **kwargs,\n) -&gt; float:\n    \"\"\"Estimates uncertainty using the gap between the top-k sequence probabilities.\n\n    The method computes sequence-level probabilities from the sampled responses,\n    identifies the `k` highest probabilities, and returns a normalized uncertainty\n    score as `1 - (gap between top-1 and top-k probabilities)`.\n\n    A smaller gap between top-k and top-1 implies higher uncertainty (less confident top choice),\n    while a large gap suggests stronger model confidence.\n\n    Args:\n        samples (List[LLMOutput]): A list of LLM outputs containing log-probabilities.\n        seq_prob_mode (SeqProbMode, optional): Method for combining token log-probabilities into\n            a single sequence probability. Defaults to `SeqProbMode.PROD`.\n        k (int, optional): The number of top probabilities to compare. Must be &gt;= 2. Defaults to 2.\n        **kwargs: Additional arguments for extensibility (unused).\n\n    Returns:\n        float: A normalized uncertainty score based on the gap between top-1 and top-k probabilities.\n\n    Raises:\n        ValueError: If `k` is less than 2.\n        AssertionError: If any sample does not contain log-probabilities.\n    \"\"\"\n    if k &lt; 2:\n        raise ValueError(\"k should &gt;= 2\")\n    assert all(s.logprobs is not None for s in samples.samples)\n\n    logit_samples = [s.logprobs for s in samples.samples]\n    sequence_probs = [\n        self.compute_sequence_probability(logits, seq_prob_mode)\n        for logits in logit_samples\n    ]\n    sorted_seq_probs = sorted(sequence_probs)\n    gap = sorted_seq_probs[-1] - sorted_seq_probs[-k]\n    return 1 - gap\n</code></pre>"},{"location":"reference/#luq.methods.construct_nli_table","title":"<code>construct_nli_table(samples, nli_model)</code>","text":"<p>Constructs a table of NLI results for all pairs of generated samples.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>LLMSamples</code> <p>The generated language model outputs.</p> required <code>nli_model</code> <code>NLIWrapper</code> <p>An NLI model wrapper used to evaluate relationships between outputs.</p> required <p>Returns:</p> Name Type Description <code>NLITable</code> <code>NLITable</code> <p>A dictionary mapping (answer1, answer2) pairs to NLIOutput results.</p> Source code in <code>luq/models/nli.py</code> <pre><code>def construct_nli_table(samples: LLMSamples, nli_model: NLIWrapper) -&gt; NLITable:\n    \"\"\"\n    Constructs a table of NLI results for all pairs of generated samples.\n\n    Args:\n        samples (LLMSamples): The generated language model outputs.\n        nli_model (NLIWrapper): An NLI model wrapper used to evaluate relationships between outputs.\n\n    Returns:\n        NLITable: A dictionary mapping (answer1, answer2) pairs to NLIOutput results.\n    \"\"\"\n    result = {}\n    for i, s1 in enumerate(samples.samples):\n        for s2 in samples.samples:\n            answer1, answer2 = s1.answer, s2.answer\n            if (answer1, answer2) in result:\n                continue\n            nli_output: NLIOutput = nli_model(answer1, answer2, params=samples.params)\n            result[(answer1, answer2)] = nli_output\n    return result\n</code></pre>"},{"location":"reference/#luq.methods.entropy","title":"<code>entropy(probabilities)</code>","text":"<p>Computes the entropy of a probability distribution.</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Union[List[float], Tensor]</code> <p>A list or tensor of probabilities. The probabilities should sum to 1 and represent a valid distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed entropy as a scalar tensor value.</p> Notes <p>Adds a small epsilon (1e-9) to probabilities to avoid log(0).</p> Source code in <code>luq/utils/utils.py</code> <pre><code>def entropy(probabilities: Union[List[float], torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the entropy of a probability distribution.\n\n    Args:\n        probabilities (Union[List[float], torch.Tensor]): A list or tensor of probabilities.\n            The probabilities should sum to 1 and represent a valid distribution.\n\n    Returns:\n        torch.Tensor: The computed entropy as a scalar tensor value.\n\n    Notes:\n        Adds a small epsilon (1e-9) to probabilities to avoid log(0).\n    \"\"\"\n    probabilities = (\n        torch.tensor(probabilities, dtype=torch.float32)\n        if isinstance(probabilities, list)\n        else probabilities\n    )\n    entropy_value = -torch.sum(probabilities * torch.log(probabilities + 1e-9))\n    return entropy_value.item()\n</code></pre>"},{"location":"reference/#luq.methods.hard_nli_clustering","title":"<code>hard_nli_clustering(samples, nli_table)</code>","text":"<p>Performs hard clustering of samples based on mutual entailment using NLI results.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>LLMSamples</code> <p>The list of LLM-generated samples.</p> required <code>nli_table</code> <code>NLITable</code> <p>A dictionary of NLI outputs between sample pairs.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: A list of cluster assignments (by index) for each sample.</p> Source code in <code>luq/models/nli.py</code> <pre><code>def hard_nli_clustering(samples: LLMSamples, nli_table: NLITable) -&gt; T.List[int]:\n    \"\"\"\n    Performs hard clustering of samples based on mutual entailment using NLI results.\n\n    Args:\n        samples (LLMSamples): The list of LLM-generated samples.\n        nli_table (NLITable): A dictionary of NLI outputs between sample pairs.\n\n    Returns:\n        List[int]: A list of cluster assignments (by index) for each sample.\n    \"\"\"\n    clusters = [None] * len(samples.samples)\n    last_cluster = 0\n    for i, s1 in enumerate(samples.samples):\n        if clusters[i] is None:\n            clusters[i] = last_cluster\n            last_cluster += 1\n        for j, s2 in enumerate(samples.samples[i + 1 :], i + 1):\n            if clusters[j] is not None:\n                continue\n            if (\n                nli_table[(s1.answer, s2.answer)].cls == NLIResult.ENTAILMENT\n                and nli_table[(s2.answer, s1.answer)].cls == NLIResult.ENTAILMENT\n            ):\n                clusters[j] = clusters[i]\n    return clusters\n</code></pre>"},{"location":"reference/#luq.methods.von_neumann_entropy","title":"<code>von_neumann_entropy(rho)</code>","text":"<p>Compute the von Neumann entropy of a density matrix.</p> The von Neumann entropy is defined as <p>S(\u03c1) = -Tr(\u03c1 log(\u03c1))</p> <p>Parameters:</p> Name Type Description Default <code>rho</code> <code>Tensor</code> <p>A Hermitian, positive semi-definite matrix (density matrix).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scalar tensor representing the entropy.</p> Source code in <code>luq/methods/kernel_utils.py</code> <pre><code>def von_neumann_entropy(rho: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the von Neumann entropy of a density matrix.\n\n    The von Neumann entropy is defined as:\n        S(\u03c1) = -Tr(\u03c1 log(\u03c1))\n\n    Args:\n        rho (torch.Tensor): A Hermitian, positive semi-definite matrix (density matrix).\n\n    Returns:\n        torch.Tensor: Scalar tensor representing the entropy.\n    \"\"\"\n    # Compute eigenvalues (ensuring they are real since rho should be Hermitian)\n    eigenvalues = torch.linalg.eigvalsh(rho)\n\n    # Avoid log(0) by masking zero values\n    nonzero_eigenvalues = eigenvalues[eigenvalues &gt; 0]\n\n    # Compute entropy\n    entropy = -torch.sum(nonzero_eigenvalues * torch.log(nonzero_eigenvalues))\n\n    return entropy\n</code></pre>"},{"location":"reference/#models","title":"Models","text":""},{"location":"reference/#luq.models.AzureCustomGPT4Wrapper","title":"<code>AzureCustomGPT4Wrapper</code>","text":"<p>Wrapper for Azure-hosted GPT-4 model using OpenAI-compatible API.</p> Source code in <code>luq/models/llm.py</code> <pre><code>class AzureCustomGPT4Wrapper:\n    \"\"\"\n    Wrapper for Azure-hosted GPT-4 model using OpenAI-compatible API.\n    \"\"\"\n    def __init__(self, openai_endpoint_url, api_key):\n        \"\"\"\n        Initializes the Azure GPT-4 wrapper.\n\n        Args:\n            openai_endpoint_url (str): The base URL of the Azure OpenAI endpoint.\n            api_key (str): Azure API key for authentication.\n        \"\"\"\n        self.openai_endpoint_url = openai_endpoint_url\n        self.client = OpenAI(\n            base_url=openai_endpoint_url,\n            api_key=False,\n            default_headers={\n                \"Ocp-Apim-Subscription-Key\": api_key,\n            },\n            http_client=httpx.Client(\n                event_hooks={\"request\": [functools.partial(update_base_url, openai_endpoint_url=openai_endpoint_url)]}\n            ),\n        )\n\n    def __call__(self, input: str) -&gt; LLMOutput:\n        \"\"\"\n        Generates a response using the Azure-hosted GPT-4 model.\n\n        Args:\n            input (str): User input prompt.\n\n        Returns:\n            LLMOutput: Generated answer and optional log probabilities.\n        \"\"\"\n        kwargs = {\n            \"model\": \"no_effect\",  # Replace with your actual deployment name\n            \"logprobs\": True,\n            \"messages\": [{\"role\": \"user\", \"content\": input}],\n            \"top_logprobs\": 5,\n        }\n        response = self.client.chat.completions.create(**kwargs)\n        content = response.choices[0].message.content\n        token_logprobs = response.choices[0].logprobs.token_logprobs\n        if token_logprobs is not None:\n            logprobs_tensor = torch.tensor(token_logprobs, dtype=torch.float32)\n        else:\n            logprobs_tensor = None\n        return LLMOutput(answer=content, logprobs=logprobs_tensor)\n</code></pre>"},{"location":"reference/#luq.models.AzureCustomGPT4Wrapper.__call__","title":"<code>__call__(input)</code>","text":"<p>Generates a response using the Azure-hosted GPT-4 model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>User input prompt.</p> required <p>Returns:</p> Name Type Description <code>LLMOutput</code> <code>LLMOutput</code> <p>Generated answer and optional log probabilities.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(self, input: str) -&gt; LLMOutput:\n    \"\"\"\n    Generates a response using the Azure-hosted GPT-4 model.\n\n    Args:\n        input (str): User input prompt.\n\n    Returns:\n        LLMOutput: Generated answer and optional log probabilities.\n    \"\"\"\n    kwargs = {\n        \"model\": \"no_effect\",  # Replace with your actual deployment name\n        \"logprobs\": True,\n        \"messages\": [{\"role\": \"user\", \"content\": input}],\n        \"top_logprobs\": 5,\n    }\n    response = self.client.chat.completions.create(**kwargs)\n    content = response.choices[0].message.content\n    token_logprobs = response.choices[0].logprobs.token_logprobs\n    if token_logprobs is not None:\n        logprobs_tensor = torch.tensor(token_logprobs, dtype=torch.float32)\n    else:\n        logprobs_tensor = None\n    return LLMOutput(answer=content, logprobs=logprobs_tensor)\n</code></pre>"},{"location":"reference/#luq.models.AzureCustomGPT4Wrapper.__init__","title":"<code>__init__(openai_endpoint_url, api_key)</code>","text":"<p>Initializes the Azure GPT-4 wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>openai_endpoint_url</code> <code>str</code> <p>The base URL of the Azure OpenAI endpoint.</p> required <code>api_key</code> <code>str</code> <p>Azure API key for authentication.</p> required Source code in <code>luq/models/llm.py</code> <pre><code>def __init__(self, openai_endpoint_url, api_key):\n    \"\"\"\n    Initializes the Azure GPT-4 wrapper.\n\n    Args:\n        openai_endpoint_url (str): The base URL of the Azure OpenAI endpoint.\n        api_key (str): Azure API key for authentication.\n    \"\"\"\n    self.openai_endpoint_url = openai_endpoint_url\n    self.client = OpenAI(\n        base_url=openai_endpoint_url,\n        api_key=False,\n        default_headers={\n            \"Ocp-Apim-Subscription-Key\": api_key,\n        },\n        http_client=httpx.Client(\n            event_hooks={\"request\": [functools.partial(update_base_url, openai_endpoint_url=openai_endpoint_url)]}\n        ),\n    )\n</code></pre>"},{"location":"reference/#luq.models.BatchLLMWrapper","title":"<code>BatchLLMWrapper</code>","text":"<p>Abstract class for batch LLM interfaces that return multiple LLM outputs.</p> Source code in <code>luq/models/llm.py</code> <pre><code>class BatchLLMWrapper:\n    \"\"\"\n    Abstract class for batch LLM interfaces that return multiple LLM outputs.\n    \"\"\"\n    def __call__(self, *args, **kwargs) -&gt; T.List[LLMOutput]:\n        \"\"\"\n        Generate multiple responses from an LLM.\n\n        Returns:\n            List[LLMOutput]: List of outputs from the model.\n\n        Raises:\n            NotImplementedError: If not implemented in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.models.BatchLLMWrapper.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Generate multiple responses from an LLM.</p> <p>Returns:</p> Type Description <code>List[LLMOutput]</code> <p>List[LLMOutput]: List of outputs from the model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented in a subclass.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; T.List[LLMOutput]:\n    \"\"\"\n    Generate multiple responses from an LLM.\n\n    Returns:\n        List[LLMOutput]: List of outputs from the model.\n\n    Raises:\n        NotImplementedError: If not implemented in a subclass.\n    \"\"\"\n    raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.models.ClaudeWrapper","title":"<code>ClaudeWrapper</code>","text":"<p>Wrapper for Anthropic's Claude models.</p> Source code in <code>luq/models/llm.py</code> <pre><code>class ClaudeWrapper:\n    \"\"\"\n    Wrapper for Anthropic's Claude models.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes the ClaudeWrapper.\n\n        Args:\n            api_key (str): Anthropic API key.\n        \"\"\"\n        self.client = Anthropic(api_key=api_key)\n\n    def __call__(self, prompt: str, model: str = \"claude-3-opus-20240229\", temperature: float = 1.0, max_tokens: int = 1024) -&gt; LLMOutput:\n        \"\"\"\n        Generates a response from Claude with optional parameters.\n\n        Note:\n            Claude's API currently does not support returning log probabilities.\n\n        Args:\n            prompt (str): Input prompt for Claude.\n            model (str): Claude model to use.\n            temperature (float): Sampling temperature.\n            max_tokens (int): Maximum number of tokens to generate.\n\n        Returns:\n            LLMOutput: Generated answer text.\n        \"\"\"\n        # Anthropic API does not currently support logprobs in the chat API.\n        try:\n            response = self.client.messages.create(\n                model=model,\n                max_tokens=max_tokens,\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=temperature,\n            )\n\n            # Extract text response\n            text = response.content[0].text if response.content else \"\"\n            return LLMOutput(answer=text)\n\n        except Exception as e:\n            raise RuntimeError(f\"Claude API call failed: {e}\")\n</code></pre>"},{"location":"reference/#luq.models.ClaudeWrapper.__call__","title":"<code>__call__(prompt, model='claude-3-opus-20240229', temperature=1.0, max_tokens=1024)</code>","text":"<p>Generates a response from Claude with optional parameters.</p> Note <p>Claude's API currently does not support returning log probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Input prompt for Claude.</p> required <code>model</code> <code>str</code> <p>Claude model to use.</p> <code>'claude-3-opus-20240229'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>1024</code> <p>Returns:</p> Name Type Description <code>LLMOutput</code> <code>LLMOutput</code> <p>Generated answer text.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(self, prompt: str, model: str = \"claude-3-opus-20240229\", temperature: float = 1.0, max_tokens: int = 1024) -&gt; LLMOutput:\n    \"\"\"\n    Generates a response from Claude with optional parameters.\n\n    Note:\n        Claude's API currently does not support returning log probabilities.\n\n    Args:\n        prompt (str): Input prompt for Claude.\n        model (str): Claude model to use.\n        temperature (float): Sampling temperature.\n        max_tokens (int): Maximum number of tokens to generate.\n\n    Returns:\n        LLMOutput: Generated answer text.\n    \"\"\"\n    # Anthropic API does not currently support logprobs in the chat API.\n    try:\n        response = self.client.messages.create(\n            model=model,\n            max_tokens=max_tokens,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=temperature,\n        )\n\n        # Extract text response\n        text = response.content[0].text if response.content else \"\"\n        return LLMOutput(answer=text)\n\n    except Exception as e:\n        raise RuntimeError(f\"Claude API call failed: {e}\")\n</code></pre>"},{"location":"reference/#luq.models.ClaudeWrapper.__init__","title":"<code>__init__(api_key)</code>","text":"<p>Initializes the ClaudeWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Anthropic API key.</p> required Source code in <code>luq/models/llm.py</code> <pre><code>def __init__(self, api_key: str):\n    \"\"\"\n    Initializes the ClaudeWrapper.\n\n    Args:\n        api_key (str): Anthropic API key.\n    \"\"\"\n    self.client = Anthropic(api_key=api_key)\n</code></pre>"},{"location":"reference/#luq.models.HFLLMWrapper","title":"<code>HFLLMWrapper</code>","text":"<p>               Bases: <code>LLMWrapper</code></p> <p>Hugging Face LLM wrapper using a tokenizer and model from the transformers library.</p> Source code in <code>luq/models/llm.py</code> <pre><code>class HFLLMWrapper(LLMWrapper):\n    \"\"\"\n    Hugging Face LLM wrapper using a tokenizer and model from the transformers library.\n    \"\"\"\n    def __init__(\n        self, tokenizer: transformers.AutoTokenizer, model: transformers.PreTrainedModel\n    ):\n        \"\"\"\n        Initializes the HFLLMWrapper with a tokenizer and model.\n\n        Args:\n            tokenizer (transformers.AutoTokenizer): A Hugging Face tokenizer instance.\n            model (transformers.PreTrainedModel): A Hugging Face model instance.\n\n        Raises:\n            ValueError: If the tokenizer or model is not a valid Hugging Face object.\n        \"\"\"\n        if isinstance(tokenizer, transformers.PreTrainedTokenizerBase):\n            self.tokenizer = tokenizer\n        else:\n            raise ValueError(\"Requires a text generation pipeline from transformers\")\n        if isinstance(model, transformers.PreTrainedModel):\n            self.model = model\n        else:\n            raise ValueError(\"Requires a text generation pipeline from transformers\")\n\n    def __call__(\n        self,\n        prompt: str,\n        temperature: float = 1.0,\n        max_new_tokens=1024,\n        *args,\n        **kwargs\n    ) -&gt; LLMOutput:\n        \"\"\"\n        Generates a response from the Hugging Face model.\n\n        Args:\n            prompt (str): The prompt to send to the model.\n            temperature (float): Sampling temperature.\n            max_new_tokens (int): Maximum number of tokens to generate.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            LLMOutput: The generated text and associated log probabilities.\n        \"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=50,\n                return_dict_in_generate=True,\n                output_scores=True,\n                do_sample=True,\n                temperature=1.0,\n            )\n        generated_ids = outputs.sequences[0]\n        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n        token_scores = outputs.scores\n        generated_tokens = generated_ids[len(inputs[\"input_ids\"][0]) :]\n\n        logprobs = []\n        for i, token_id in enumerate(generated_tokens):\n            logits = token_scores[i][0]\n            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n            token_logprob = log_probs[token_id].item()\n            logprobs.append((self.tokenizer.decode([token_id]), token_logprob))\n\n        # logprobs is a list of pairs (token, logprob)\n        logprobs = [el[1] for el in logprobs]\n        return LLMOutput(\n            answer=generated_text,\n            logprobs=torch.tensor(logprobs, device=self.model.device),\n        )\n</code></pre>"},{"location":"reference/#luq.models.HFLLMWrapper.__call__","title":"<code>__call__(prompt, temperature=1.0, max_new_tokens=1024, *args, **kwargs)</code>","text":"<p>Generates a response from the Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the model.</p> required <code>temperature</code> <code>float</code> <p>Sampling temperature.</p> <code>1.0</code> <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> <code>1024</code> <code>*args</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLMOutput</code> <code>LLMOutput</code> <p>The generated text and associated log probabilities.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(\n    self,\n    prompt: str,\n    temperature: float = 1.0,\n    max_new_tokens=1024,\n    *args,\n    **kwargs\n) -&gt; LLMOutput:\n    \"\"\"\n    Generates a response from the Hugging Face model.\n\n    Args:\n        prompt (str): The prompt to send to the model.\n        temperature (float): Sampling temperature.\n        max_new_tokens (int): Maximum number of tokens to generate.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LLMOutput: The generated text and associated log probabilities.\n    \"\"\"\n    inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n\n    with torch.no_grad():\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=50,\n            return_dict_in_generate=True,\n            output_scores=True,\n            do_sample=True,\n            temperature=1.0,\n        )\n    generated_ids = outputs.sequences[0]\n    generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n    token_scores = outputs.scores\n    generated_tokens = generated_ids[len(inputs[\"input_ids\"][0]) :]\n\n    logprobs = []\n    for i, token_id in enumerate(generated_tokens):\n        logits = token_scores[i][0]\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        token_logprob = log_probs[token_id].item()\n        logprobs.append((self.tokenizer.decode([token_id]), token_logprob))\n\n    # logprobs is a list of pairs (token, logprob)\n    logprobs = [el[1] for el in logprobs]\n    return LLMOutput(\n        answer=generated_text,\n        logprobs=torch.tensor(logprobs, device=self.model.device),\n    )\n</code></pre>"},{"location":"reference/#luq.models.HFLLMWrapper.__init__","title":"<code>__init__(tokenizer, model)</code>","text":"<p>Initializes the HFLLMWrapper with a tokenizer and model.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>AutoTokenizer</code> <p>A Hugging Face tokenizer instance.</p> required <code>model</code> <code>PreTrainedModel</code> <p>A Hugging Face model instance.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tokenizer or model is not a valid Hugging Face object.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __init__(\n    self, tokenizer: transformers.AutoTokenizer, model: transformers.PreTrainedModel\n):\n    \"\"\"\n    Initializes the HFLLMWrapper with a tokenizer and model.\n\n    Args:\n        tokenizer (transformers.AutoTokenizer): A Hugging Face tokenizer instance.\n        model (transformers.PreTrainedModel): A Hugging Face model instance.\n\n    Raises:\n        ValueError: If the tokenizer or model is not a valid Hugging Face object.\n    \"\"\"\n    if isinstance(tokenizer, transformers.PreTrainedTokenizerBase):\n        self.tokenizer = tokenizer\n    else:\n        raise ValueError(\"Requires a text generation pipeline from transformers\")\n    if isinstance(model, transformers.PreTrainedModel):\n        self.model = model\n    else:\n        raise ValueError(\"Requires a text generation pipeline from transformers\")\n</code></pre>"},{"location":"reference/#luq.models.LLMOutput","title":"<code>LLMOutput</code>  <code>dataclass</code>","text":"<p>Represents the output of a language model.</p> <p>Attributes:</p> Name Type Description <code>answer</code> <code>str</code> <p>The generated text answer from the language model.</p> <code>logprobs</code> <code>Tensor | None</code> <p>Optional tensor containing the log probabilities associated with the generated tokens.</p> Source code in <code>luq/models/llm.py</code> <pre><code>@dataclass\nclass LLMOutput:\n    \"\"\"\n    Represents the output of a language model.\n\n    Attributes:\n        answer (str): The generated text answer from the language model.\n        logprobs (torch.Tensor | None): Optional tensor containing the log probabilities\n            associated with the generated tokens.\n    \"\"\"\n    answer: str\n    logprobs: torch.Tensor | None = None  # list of logprobs\n</code></pre>"},{"location":"reference/#luq.models.LLMSamples","title":"<code>LLMSamples</code>  <code>dataclass</code>","text":"<p>Contains multiple samples generated by a language model along with metadata.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <code>List[LLMOutput]</code> <p>A list of multiple LLMOutput samples.</p> <code>answer</code> <code>LLMOutput</code> <p>The selected or final answer output.</p> <code>params</code> <code>Dict[str, Any]</code> <p>Parameters used to generate the samples.</p> Source code in <code>luq/models/llm.py</code> <pre><code>@dataclass\nclass LLMSamples:\n    \"\"\"\n    Contains multiple samples generated by a language model along with metadata.\n\n    Attributes:\n        samples (List[LLMOutput]): A list of multiple LLMOutput samples.\n        answer (LLMOutput): The selected or final answer output.\n        params (Dict[str, Any]): Parameters used to generate the samples.\n    \"\"\"\n    samples: T.List[LLMOutput]\n    answer: LLMOutput\n    params: T.Dict[str, T.Any]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of samples generated.\n\n        Returns:\n            int: The count of samples.\n        \"\"\"\n        return len(self.samples)\n</code></pre>"},{"location":"reference/#luq.models.LLMSamples.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples generated.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of samples.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of samples generated.\n\n    Returns:\n        int: The count of samples.\n    \"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"reference/#luq.models.LLMWrapper","title":"<code>LLMWrapper</code>","text":"Source code in <code>luq/models/llm.py</code> <pre><code>class LLMWrapper:\n    def __call__(self, *args, **kwargs) -&gt; LLMOutput:\n        \"\"\"\n        Abstract base wrapper for language model interfaces.\n\n        This class is meant to be subclassed to implement specific LLM calls.\n        \"\"\"\n        raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.models.LLMWrapper.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Abstract base wrapper for language model interfaces.</p> <p>This class is meant to be subclassed to implement specific LLM calls.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; LLMOutput:\n    \"\"\"\n    Abstract base wrapper for language model interfaces.\n\n    This class is meant to be subclassed to implement specific LLM calls.\n    \"\"\"\n    raise NotImplementedError(\"__call__ should be implemented for your LLM\")\n</code></pre>"},{"location":"reference/#luq.models.all_logits_present","title":"<code>all_logits_present(samples)</code>","text":"<p>Checks whether all LLM outputs contain log probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[LLMOutput]</code> <p>A list of LLM output samples.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all samples include logprobs, False otherwise.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def all_logits_present(samples: T.List[LLMOutput]) -&gt; bool:\n    \"\"\"\n    Checks whether all LLM outputs contain log probabilities.\n\n    Args:\n        samples (List[LLMOutput]): A list of LLM output samples.\n\n    Returns:\n        bool: True if all samples include logprobs, False otherwise.\n    \"\"\"\n    return all(sample.logprobs is not None for sample in samples)\n</code></pre>"},{"location":"reference/#luq.models.generate_n_samples_and_answer","title":"<code>generate_n_samples_and_answer(llm, prompt, temp_gen=1.0, temp_answer=0.1, top_p_gen=0.9, top_k_gen=16, top_p_ans=0.7, top_k_ans=4, n_samples=10)</code>","text":"<p>Generates multiple LLM samples and a single final answer using specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLMWrapper</code> <p>The language model wrapper to use.</p> required <code>prompt</code> <code>str</code> <p>The prompt to pass to the LLM.</p> required <code>temp_gen</code> <code>float</code> <p>Temperature for generating samples.</p> <code>1.0</code> <code>temp_answer</code> <code>float</code> <p>Temperature for the final answer.</p> <code>0.1</code> <code>top_p_gen</code> <code>float</code> <p>Nucleus sampling parameter for generation.</p> <code>0.9</code> <code>top_k_gen</code> <code>int</code> <p>Top-k sampling parameter for generation.</p> <code>16</code> <code>top_p_ans</code> <code>float</code> <p>Nucleus sampling parameter for answer.</p> <code>0.7</code> <code>top_k_ans</code> <code>int</code> <p>Top-k sampling parameter for answer.</p> <code>4</code> <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>LLMSamples</code> <code>LLMSamples</code> <p>A collection of generated samples, the final answer, and parameters used.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If <code>llm</code> is not an instance of LLMWrapper.</p> Source code in <code>luq/models/llm.py</code> <pre><code>def generate_n_samples_and_answer(\n    llm: LLMWrapper,\n    prompt: str,\n    temp_gen: float = 1.0,\n    temp_answer: float = 0.1,\n    top_p_gen: float = 0.9,\n    top_k_gen: float = 16,\n    top_p_ans: float = 0.7,\n    top_k_ans: float = 4,\n    n_samples: int = 10,\n) -&gt; LLMSamples:\n    \"\"\"\n    Generates multiple LLM samples and a single final answer using specified parameters.\n\n    Args:\n        llm (LLMWrapper): The language model wrapper to use.\n        prompt (str): The prompt to pass to the LLM.\n        temp_gen (float): Temperature for generating samples.\n        temp_answer (float): Temperature for the final answer.\n        top_p_gen (float): Nucleus sampling parameter for generation.\n        top_k_gen (int): Top-k sampling parameter for generation.\n        top_p_ans (float): Nucleus sampling parameter for answer.\n        top_k_ans (int): Top-k sampling parameter for answer.\n        n_samples (int): Number of samples to generate.\n\n    Returns:\n        LLMSamples: A collection of generated samples, the final answer, and parameters used.\n\n    Raises:\n        NotImplementedError: If `llm` is not an instance of LLMWrapper.\n    \"\"\"\n    if isinstance(llm, LLMWrapper):\n        sampled_answers = [\n            llm(prompt, temperature=temp_gen, top_p=top_p_gen, top_k=top_k_gen)\n            for _ in range(n_samples)\n        ]\n        answer = llm(prompt, temperature=temp_gen, top_p=top_p_gen, top_k=top_k_gen)\n        params = {\n            \"prompt\": prompt,\n            \"temp_gen\": temp_gen,\n            \"temp_answer\": temp_answer,\n            \"top_p_gen\": top_p_gen,\n            \"top_k_gen\": top_k_gen,\n            \"top_p_ans\": top_p_ans,\n            \"top_k_ans\": top_k_ans,\n            \"n_samples\": n_samples,\n            \"llm\": str(llm),\n        }\n        return LLMSamples(samples=sampled_answers, answer=answer, params=params)\n    else:\n        raise NotImplementedError(\n            \"generation is currently supported only for LLMWrapper\"\n        )\n</code></pre>"},{"location":"reference/#luq.models.update_base_url","title":"<code>update_base_url(request, openai_endpoint_url)</code>","text":"<p>Modifies the base URL of an OpenAI request to route to a custom Azure endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTPX request object to be modified.</p> required <code>openai_endpoint_url</code> <code>str</code> <p>The Azure OpenAI endpoint path to use.</p> required Source code in <code>luq/models/llm.py</code> <pre><code>def update_base_url(request: httpx.Request, openai_endpoint_url: str) -&gt; None:\n    \"\"\"\n    Modifies the base URL of an OpenAI request to route to a custom Azure endpoint.\n\n    Args:\n        request (httpx.Request): The HTTPX request object to be modified.\n        openai_endpoint_url (str): The Azure OpenAI endpoint path to use.\n    \"\"\"\n    if request.url.path == \"/chat/completions\":\n        request.url = request.url.copy_with(path=openai_endpoint_url)\n</code></pre>"},{"location":"reference/#datasets","title":"Datasets","text":""},{"location":"reference/#luq.datasets.GenerationDataset","title":"<code>GenerationDataset</code>","text":"<p>               Bases: <code>DatasetDict</code></p> Source code in <code>luq/datasets/dataset.py</code> <pre><code>class GenerationDataset(DatasetDict):\n    def __init__(self, data_path: str = None, arrow_table=None):\n        \"\"\"\n        Initializes the dataset object.\n        :param data_path: Path to the JSON file containing the dataset.\n        \"\"\"\n        if data_path is not None:\n            dataset_dict = self.load_from_json(data_path)\n            super().__init__(dataset_dict)\n        elif arrow_table is not None:\n            dataset = Dataset(arrow_table)\n            super().__init__({\"train\": dataset})\n        else:\n            raise ValueError(\"Either data_path or arrow_table must be provided\")\n\n    @staticmethod\n    def load_from_json(data_path: str) -&gt; Dataset:\n        \"\"\"\n        Loads the dataset from a JSON file and converts it into a Hugging Face Dataset object.\n        \"\"\"\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            raw_data = json.load(f)\n\n        split_datasets = {}\n        for split, items in raw_data[\"data\"].items():\n            processed_items = [\n                {\n                    \"question\": item[\"question\"],\n                    \"samples\": item[\"samples\"],\n                    \"logprobs\": item.get(\"logprobs\", []),\n                    \"answer\": item[\"answer\"],\n                    \"gt_answer\": item[\"gt_answer\"],\n                    \"accuracy\": item.get(\"accuracy\"),\n                }\n                for item in items\n            ]\n            split_datasets[split] = Dataset.from_list(processed_items)\n        return DatasetDict(split_datasets)\n\n    def split_dataset(self, train_size: float = 0.8) -&gt; Dict[str, \"GenerationDataset\"]:\n        \"\"\"\n        Splits the dataset into train and test sets.\n        :param train_size: Proportion of the dataset to include in the train split.\n        :return: Dictionary containing train and test GenerationDataset objects\n        \"\"\"\n        splits = super().train_test_split(train_size=train_size)\n        return {\n            \"train\": GenerationDataset(arrow_table=splits[\"train\"]._data),\n            \"test\": GenerationDataset(arrow_table=splits[\"test\"]._data),\n        }\n\n    @classmethod\n    def from_dataset(cls, dataset: Dataset) -&gt; \"GenerationDataset\":\n        \"\"\"\n        Creates a GenerationDataset from a regular Dataset object\n        \"\"\"\n        return cls(arrow_table=dataset._data)\n</code></pre>"},{"location":"reference/#luq.datasets.GenerationDataset.__init__","title":"<code>__init__(data_path=None, arrow_table=None)</code>","text":"<p>Initializes the dataset object. :param data_path: Path to the JSON file containing the dataset.</p> Source code in <code>luq/datasets/dataset.py</code> <pre><code>def __init__(self, data_path: str = None, arrow_table=None):\n    \"\"\"\n    Initializes the dataset object.\n    :param data_path: Path to the JSON file containing the dataset.\n    \"\"\"\n    if data_path is not None:\n        dataset_dict = self.load_from_json(data_path)\n        super().__init__(dataset_dict)\n    elif arrow_table is not None:\n        dataset = Dataset(arrow_table)\n        super().__init__({\"train\": dataset})\n    else:\n        raise ValueError(\"Either data_path or arrow_table must be provided\")\n</code></pre>"},{"location":"reference/#luq.datasets.GenerationDataset.from_dataset","title":"<code>from_dataset(dataset)</code>  <code>classmethod</code>","text":"<p>Creates a GenerationDataset from a regular Dataset object</p> Source code in <code>luq/datasets/dataset.py</code> <pre><code>@classmethod\ndef from_dataset(cls, dataset: Dataset) -&gt; \"GenerationDataset\":\n    \"\"\"\n    Creates a GenerationDataset from a regular Dataset object\n    \"\"\"\n    return cls(arrow_table=dataset._data)\n</code></pre>"},{"location":"reference/#luq.datasets.GenerationDataset.load_from_json","title":"<code>load_from_json(data_path)</code>  <code>staticmethod</code>","text":"<p>Loads the dataset from a JSON file and converts it into a Hugging Face Dataset object.</p> Source code in <code>luq/datasets/dataset.py</code> <pre><code>@staticmethod\ndef load_from_json(data_path: str) -&gt; Dataset:\n    \"\"\"\n    Loads the dataset from a JSON file and converts it into a Hugging Face Dataset object.\n    \"\"\"\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        raw_data = json.load(f)\n\n    split_datasets = {}\n    for split, items in raw_data[\"data\"].items():\n        processed_items = [\n            {\n                \"question\": item[\"question\"],\n                \"samples\": item[\"samples\"],\n                \"logprobs\": item.get(\"logprobs\", []),\n                \"answer\": item[\"answer\"],\n                \"gt_answer\": item[\"gt_answer\"],\n                \"accuracy\": item.get(\"accuracy\"),\n            }\n            for item in items\n        ]\n        split_datasets[split] = Dataset.from_list(processed_items)\n    return DatasetDict(split_datasets)\n</code></pre>"},{"location":"reference/#luq.datasets.GenerationDataset.split_dataset","title":"<code>split_dataset(train_size=0.8)</code>","text":"<p>Splits the dataset into train and test sets. :param train_size: Proportion of the dataset to include in the train split. :return: Dictionary containing train and test GenerationDataset objects</p> Source code in <code>luq/datasets/dataset.py</code> <pre><code>def split_dataset(self, train_size: float = 0.8) -&gt; Dict[str, \"GenerationDataset\"]:\n    \"\"\"\n    Splits the dataset into train and test sets.\n    :param train_size: Proportion of the dataset to include in the train split.\n    :return: Dictionary containing train and test GenerationDataset objects\n    \"\"\"\n    splits = super().train_test_split(train_size=train_size)\n    return {\n        \"train\": GenerationDataset(arrow_table=splits[\"train\"]._data),\n        \"test\": GenerationDataset(arrow_table=splits[\"test\"]._data),\n    }\n</code></pre>"},{"location":"reference/#utility-functions","title":"Utility Functions","text":""},{"location":"reference/#luq.utils.SeqProbMode","title":"<code>SeqProbMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for modes of combining token probabilities in a sequence.</p> <p>Attributes:</p> Name Type Description <code>PROD</code> <p>Use the product of probabilities.</p> <code>AVG</code> <p>Use the average of probabilities.</p> Source code in <code>luq/utils/utils.py</code> <pre><code>class SeqProbMode(Enum):\n    \"\"\"\n    Enumeration for modes of combining token probabilities in a sequence.\n\n    Attributes:\n        PROD: Use the product of probabilities.\n        AVG: Use the average of probabilities.\n    \"\"\"\n    PROD = \"prod\"\n    AVG = \"avg\"\n</code></pre>"},{"location":"reference/#luq.utils.entropy","title":"<code>entropy(probabilities)</code>","text":"<p>Computes the entropy of a probability distribution.</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>Union[List[float], Tensor]</code> <p>A list or tensor of probabilities. The probabilities should sum to 1 and represent a valid distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The computed entropy as a scalar tensor value.</p> Notes <p>Adds a small epsilon (1e-9) to probabilities to avoid log(0).</p> Source code in <code>luq/utils/utils.py</code> <pre><code>def entropy(probabilities: Union[List[float], torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the entropy of a probability distribution.\n\n    Args:\n        probabilities (Union[List[float], torch.Tensor]): A list or tensor of probabilities.\n            The probabilities should sum to 1 and represent a valid distribution.\n\n    Returns:\n        torch.Tensor: The computed entropy as a scalar tensor value.\n\n    Notes:\n        Adds a small epsilon (1e-9) to probabilities to avoid log(0).\n    \"\"\"\n    probabilities = (\n        torch.tensor(probabilities, dtype=torch.float32)\n        if isinstance(probabilities, list)\n        else probabilities\n    )\n    entropy_value = -torch.sum(probabilities * torch.log(probabilities + 1e-9))\n    return entropy_value.item()\n</code></pre>"},{"location":"reference/#scripts_1","title":"Scripts","text":""}]}